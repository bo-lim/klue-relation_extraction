{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2646cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# import pororo\n",
    "# from pororo import Pororo\n",
    "\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bbe465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 100.0.4896\n",
      "Get LATEST chromedriver version for 100.0.4896 google-chrome\n",
      "There is no [win32] chromedriver for browser 100.0.4896 in cache\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/100.0.4896.60/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\Admin\\.wdm\\drivers\\chromedriver\\win32\\100.0.4896.60]\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_7312\\3969098803.py:6: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), chrome_options=chrome_options)\n"
     ]
    }
   ],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), chrome_options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f329fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling\n",
    "def kor_to_trans(df, trans_lang, start_index,final_index):\n",
    "    encode_sentence = []\n",
    "    encode_subject = []\n",
    "    encode_object = []\n",
    "    \n",
    "    target_present = EC.presence_of_element_located((By.XPATH, '//*[@id=\"txtTarget\"]'))\n",
    "\n",
    "    for i in tqdm(range(start_index,final_index)): \n",
    "    \n",
    "        try:\n",
    "            driver.get('https://papago.naver.com/?sk=ko&tk='+trans_lang+'&st='+df[\"sentence\"].iloc[i])\n",
    "            time.sleep(1.5)\n",
    "            element=WebDriverWait(driver, 10).until(target_present)\n",
    "            time.sleep(0.8)\n",
    "            backtrans_sentence = element.text \n",
    "\n",
    "            if (backtrans_sentence=='')|(backtrans_sentence==' '):\n",
    "                element=WebDriverWait(driver, 10).until(target_present)\n",
    "                backtrans_sentence = element.text \n",
    "                encode_sentence.append(backtrans_sentence)\n",
    "            else:\n",
    "                encode_sentence.append(backtrans_sentence)\n",
    "            \n",
    "        except:\n",
    "            encode_sentence.append('')\n",
    "            \n",
    "        try:\n",
    "            driver.get('https://papago.naver.com/?sk=ko&tk='+trans_lang+'&st='+extract_word(df['subject_entity'].iloc[i]))\n",
    "            time.sleep(1.5)\n",
    "            element=WebDriverWait(driver, 10).until(target_present)\n",
    "            time.sleep(0.8)\n",
    "            backtrans_subject = element.text \n",
    "\n",
    "            if (backtrans_subject=='')|(backtrans_subject==' '):\n",
    "                element=WebDriverWait(driver, 10).until(target_present)\n",
    "                backtrans_subject = element.text \n",
    "                encode_subject.append(backtrans_subject)\n",
    "            else:\n",
    "                encode_subject.append(backtrans_subject)\n",
    "            \n",
    "        except:\n",
    "            encode_subject.append('')\n",
    "            \n",
    "        try:\n",
    "            driver.get('https://papago.naver.com/?sk=ko&tk='+trans_lang+'&st='+extract_word(df['object_entity'].iloc[i]))\n",
    "            time.sleep(1.5)\n",
    "            element=WebDriverWait(driver, 10).until(target_present)\n",
    "            time.sleep(0.8)\n",
    "            backtrans_object = element.text \n",
    "\n",
    "            if (backtrans_object=='')|(backtrans_object==' '):\n",
    "                element=WebDriverWait(driver, 10).until(target_present)\n",
    "                backtrans_object = element.text \n",
    "                encode_object.append(backtrans_object)\n",
    "            else:\n",
    "                encode_object.append(backtrans_object)\n",
    "            \n",
    "        except:\n",
    "            encode_object.append('')\n",
    "            \n",
    "    return  encode_sentence, encode_subject, encode_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc7c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def via_trans(sentence, subject_list, object_list, start_lang, end_lang):\n",
    "    \"\"\"\n",
    "    한국어가 아닌 다른언어간 번역\n",
    "    \"\"\"\n",
    "    via_sentence = []\n",
    "    via_subject = []\n",
    "    via_object = []\n",
    "    \n",
    "    target_present = EC.presence_of_element_located((By.XPATH, '//*[@id=\"txtTarget\"]'))\n",
    "\n",
    "    for i in tqdm(range(len(sentence))): \n",
    "        if sentence[i] == '' or subject_list[i] == '' or object_list[i] == '':\n",
    "            via_sentence.append('')\n",
    "            via_subject.append('')\n",
    "            via_object.append('')\n",
    "            continue\n",
    "        try:\n",
    "            driver.get('https://papago.naver.com/?sk=' + start_lang +'&tk=' + end_lang + '&st=' + sentence[i])\n",
    "            time.sleep(1.5)\n",
    "            element=WebDriverWait(driver, 10).until(target_present)\n",
    "            time.sleep(0.8)\n",
    "            backtrans_sentence = element.text \n",
    "\n",
    "            if (backtrans_sentence=='')|(backtrans_sentence==' '):\n",
    "                element=WebDriverWait(driver, 10).until(target_present)\n",
    "                backtrans_sentence = element.text \n",
    "                via_sentence.append(backtrans_sentence)\n",
    "            else:\n",
    "                via_sentence.append(backtrans_sentence)\n",
    "            \n",
    "        except:\n",
    "            via_sentence.append('')\n",
    "            \n",
    "        try:\n",
    "            driver.get('https://papago.naver.com/?sk=' + start_lang +'&tk=' + end_lang + '&st=' + extract_word(subject_list[i]))\n",
    "            time.sleep(1.5)\n",
    "            element=WebDriverWait(driver, 10).until(target_present)\n",
    "            time.sleep(0.8)\n",
    "            backtrans_subject = element.text \n",
    "\n",
    "            if (backtrans_subject=='')|(backtrans_subject==' '):\n",
    "                element=WebDriverWait(driver, 10).until(target_present)\n",
    "                backtrans_subject = element.text \n",
    "                via_subject.append(backtrans_subject)\n",
    "            else:\n",
    "                via_subject.append(backtrans_subject)\n",
    "            \n",
    "        except:\n",
    "            via_subject.append('')\n",
    "            \n",
    "        try:\n",
    "            driver.get('https://papago.naver.com/?sk=' + start_lang +'&tk=' + end_lang + '&st=' + extract_word(object_list[i]))\n",
    "            time.sleep(1.5)\n",
    "            element=WebDriverWait(driver, 10).until(target_present)\n",
    "            time.sleep(0.8)\n",
    "            backtrans_object = element.text \n",
    "\n",
    "            if (backtrans_object=='')|(backtrans_object==' '):\n",
    "                element=WebDriverWait(driver, 10).until(target_present)\n",
    "                backtrans_object = element.text \n",
    "                via_object.append(backtrans_object)\n",
    "            else:\n",
    "                via_object.append(backtrans_object)\n",
    "            \n",
    "        except:\n",
    "            via_object.append('')\n",
    "\n",
    "            \n",
    "    return  via_sentence, via_subject, via_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f4c6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word(x):\n",
    "    return x.split(',')[0].split(':')[-1].strip()[1:-1]\n",
    "\n",
    "def extract_type(x):\n",
    "    return x.split(',')[-1].split(':')[-1].strip()[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1915e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(sentence, word):\n",
    "    length = len(word)\n",
    "    start_index = sentence.find(word)\n",
    "    end_index = start_index + length -1\n",
    "    return start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5909d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back trnaslation진행할 dataframe 생성\n",
    "original_df = pd.read_csv('C:/Users/Admin/Desktop/Boostcamp-AI-Tech/dataset/train/train.csv')\n",
    "train = original_df[original_df['label'] == 'no_relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32242c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_jp_sentence, kor_jp_subject, kor_jp_object = kor_to_trans(train, 'ja&hn=0',0,len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ef9f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_kor_sentence, jp_kor_subject, jp_kor_object = via_trans(kor_jp_sentence, kor_jp_subject, kor_jp_object, 'ja&hn=0', 'ko') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18a9ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_subject = train['subject_entity'].apply(extract_word).tolist()\n",
    "original_object = train['object_entity'].apply(extract_word).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59a4b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_count = int(32470) # 원본데이터id 0~32469\n",
    "bs_df = pd.DataFrame()  # bt(back translation) 진행한 문장에 원래 subject,object가 들어있는경우\n",
    "bs2_df = pd.DataFrame()  # bt진행한 문장에 bt진행한 subject가 들어있고 원래 object가 들어있는경우\n",
    "bs3_df = pd.DataFrame()  # bt진행한 문장에 원래 subject가 들어있고 bt진행한 object가 들어있는경우\n",
    "bs4_df = pd.DataFrame()  # bt진행한 문장에 bt진행한 subject,object 들어있는경우\n",
    "for i in range(len(train)):\n",
    "    if jp_kor_sentence[i] == '' or jp_kor_subject[i] == '' or jp_kor_object[i] == '':\n",
    "        continue\n",
    "        \n",
    "    if (original_subject[i] in jp_kor_sentence[i]) and (original_object[i] in jp_kor_sentence[i]):\n",
    "        ss,se = find_index(jp_kor_sentence[i], original_subject[i])\n",
    "        slabel = extract_type(train['subject_entity'].iloc[i])\n",
    "        sdict = \"{'word': '\"+original_subject[i]+\"',  \\'start_idx\\': \"+str(ss)+\",  \\'end_idx\\': \"+str(se)+\", 'type': '\"+slabel+\"}\"\n",
    "        \n",
    "        os,oe = find_index(jp_kor_sentence[i], original_object[i])\n",
    "        olabel = extract_type(train['object_entity'].iloc[i])\n",
    "        odict = \"{'word': '\"+original_object[i]+\"',  \\'start_idx\\': \"+str(os)+\",  \\'end_idx\\': \"+str(oe)+\", 'type': '\"+olabel+\"}\"\n",
    "        new_data = [{\n",
    "            'id' : id_count,\n",
    "            'sentence' : jp_kor_sentence[i],\n",
    "            'subject_entity' : sdict,\n",
    "            'object_entity' : odict,\n",
    "            'label' : train['label'].iloc[i],\n",
    "            'source' : train['source'].iloc[i]\n",
    "        }]\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        bs_df = pd.concat([bs_df, new_df])\n",
    "        id_count += 1\n",
    "\n",
    "\n",
    "    elif jp_kor_subject[i] in jp_kor_sentence[i] and original_object[i] in jp_kor_sentence[i]:\n",
    "        ss,se = find_index(jp_kor_sentence[i], jp_kor_subject[i])\n",
    "        slabel = extract_type(train['subject_entity'].iloc[i])\n",
    "        sdict = \"{'word': '\"+jp_kor_subject[i]+\"',  \\'start_idx\\': \"+str(ss)+\",  \\'end_idx\\': \"+str(se)+\", 'type': '\"+slabel+\"}\"\n",
    "\n",
    "        os,oe = find_index(jp_kor_sentence[i], original_object[i])\n",
    "        olabel = extract_type(train['object_entity'].iloc[i])\n",
    "        odict = \"{'word': '\"+original_object[i]+\"',  \\'start_idx\\': \"+str(os)+\",  \\'end_idx\\': \"+str(oe)+\", 'type': '\"+olabel+\"}\"\n",
    "        \n",
    "\n",
    "        new_data = [{\n",
    "            'id' : id_count,\n",
    "            'sentence' : jp_kor_sentence[i],\n",
    "            'subject_entity' : sdict,\n",
    "            'object_entity' : odict,\n",
    "            'label' : train['label'].iloc[i],\n",
    "            'source' : train['source'].iloc[i]\n",
    "        }]\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        bs2_df = pd.concat([bs2_df, new_df])\n",
    "\n",
    "    elif original_subject[i] in jp_kor_sentence[i] and jp_kor_object[i] in jp_kor_sentence[i]:\n",
    "        ss,se = find_index(jp_kor_sentence[i], original_subject[i])\n",
    "        slabel = extract_type(train['subject_entity'].iloc[i])\n",
    "        sdict = \"{'word': '\"+original_subject[i]+\"',  \\'start_idx\\': \"+str(ss)+\",  \\'end_idx\\': \"+str(se)+\", 'type': '\"+slabel+\"}\"\n",
    "\n",
    "        os,oe = find_index(jp_kor_sentence[i], jp_kor_object[i])\n",
    "        olabel = extract_type(train['object_entity'].iloc[i])\n",
    "        odict = \"{'word': '\"+jp_kor_object[i]+\"',  \\'start_idx\\': \"+str(os)+\",  \\'end_idx\\': \"+str(oe)+\", 'type': '\"+olabel+\"}\"\n",
    "\n",
    "        new_data = [{\n",
    "            'id' : id_count,\n",
    "            'sentence' : jp_kor_sentence[i],\n",
    "            'subject_entity' : sdict,\n",
    "            'object_entity' : odict,\n",
    "            'label' : train['label'].iloc[i],\n",
    "            'source' : train['source'].iloc[i]\n",
    "        }]\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        bs3_df = pd.concat([bs3_df, new_df])\n",
    "        \n",
    "    elif jp_kor_subject[i] in jp_kor_sentence[i] and jp_kor_object[i] in jp_kor_sentence[i]:\n",
    "        ss,se = find_index(jp_kor_sentence[i], jp_kor_subject[i])\n",
    "        slabel = extract_type(train['subject_entity'].iloc[i])\n",
    "        sdict = \"{'word': '\"+jp_kor_subject[i]+\"',  \\'start_idx\\': \"+str(ss)+\",  \\'end_idx\\': \"+str(se)+\", 'type': '\"+slabel+\"}\"\n",
    "\n",
    "        os,oe = find_index(jp_kor_sentence[i], jp_kor_object[i])\n",
    "        olabel = extract_type(train['object_entity'].iloc[i])\n",
    "        odict = \"{'word': '\"+jp_kor_object[i]+\"',  \\'start_idx\\': \"+str(os)+\",  \\'end_idx\\': \"+str(oe)+\", 'type': '\"+olabel+\"}\"\n",
    "\n",
    "        new_data = [{\n",
    "            'id' : id_count,\n",
    "            'sentence' : jp_kor_sentence[i],\n",
    "            'subject_entity' : sdict,\n",
    "            'object_entity' : odict,\n",
    "            'label' : train['label'].iloc[i],\n",
    "            'source' : train['source'].iloc[i]\n",
    "        }]\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        bs4_df = pd.concat([bs4_df, new_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7b7b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [bs_df, bs2_df, bs3_df, bs4_df]\n",
    "result_df = pd.concat(df_list, ignore_index=True)\n",
    "result_df = result_df.sample(frac=1)  # 행 랜덤으로 셔플\n",
    "result_df = result_df[result_df['subject_entity'] != result_df['object_entity']]\n",
    "result_df = result_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14a90b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [original_df, merge_df]\n",
    "merge_df = pd.concat(df_list, ignore_index=True)\n",
    "merge_df = merge_df.reset_index(drop=True)\n",
    "merge_df['id'] = list(range(len(merge_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2c27ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.to_csv('nore_aug.csv', index=False, encoding=\"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d0adf",
   "metadata": {},
   "source": [
    "원래 train.csv 가져와서 합치면 증강된 dataset 완성됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

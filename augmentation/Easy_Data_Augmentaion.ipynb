{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aTRkG5NdHZw"
   },
   "source": [
    "SR (Synonym Replacement): 특정 단어를 비슷한 의미의 유의어로 교체\n",
    "RI (Random Insertion): 임의의 단어를 삽입\n",
    "RS (Random Swap): 텍스트 내의 두 단어를 임의로 선정하여 서로 위치를 바꿔줌\n",
    "RD (Random Deletion): 임의의 단어를 삭제\n",
    "\n",
    "안전하게 데이터 증강을 하고 싶다면 RD, RS만을 사용하고, 데이터가 많이 필요하다싶으면 SR과 RI까지 사용하고 인간지능으로 데이터를 걸러내는 작업이 필요할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qu2jJLq6Egac"
   },
   "source": [
    "References: https://github.com/catSirup/KorEDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cWl4qSoC23a",
    "outputId": "0cadbff3-64e7-4f39-88b4-dfe62cfb8038"
   },
   "outputs": [],
   "source": [
    "pip install koeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qEzp6XyjRJHe"
   },
   "outputs": [],
   "source": [
    "from koeda import EDA\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MD5n8P1qCn8J"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1004)\n",
    "random.seed(1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "unGJS2enZJwZ"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1004)\n",
    "random.seed(1004)\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "def random_deletion(words, p):\n",
    "\tif len(words) == 1:\n",
    "\t\treturn words\n",
    "\n",
    "\tnew_words = []\n",
    "\tfor word in words:\n",
    "\t\tr = random.uniform(0, 1)\n",
    "\t\tif r > p:\n",
    "\t\t\tnew_words.append(word)\n",
    "\n",
    "\tif len(new_words) == 0:\n",
    "\t\trand_int = random.randint(0, len(words)-1)\n",
    "\t\treturn [words[rand_int]]\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "def random_swap(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tnew_words = swap_word(new_words)\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
    "\trandom_idx_2 = random_idx_1\n",
    "\tcounter = 0\n",
    "\n",
    "\twhile random_idx_2 == random_idx_1:\n",
    "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter > 3:\n",
    "\t\t\treturn new_words\n",
    "\n",
    "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "\treturn new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pV1iRrkHMWYG"
   },
   "outputs": [],
   "source": [
    "# np.random.seed(1004)\n",
    "# random.seed(1004)\n",
    "# random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsZuowr3c1UH"
   },
   "source": [
    "sr, ri는 적용시 문장의 의미가 변형되는경우가 생겨 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GhvWWHE3G1h5"
   },
   "outputs": [],
   "source": [
    "def rd_EDA(sentence, p_rd=0.1, num_aug=9):\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word != \"\"]\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\n",
    "\t# rd\n",
    "\tfor _ in range(num_aug):\n",
    "\t\ta_words = random_deletion(words, p_rd)\n",
    "\t\taugmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "\taugmented_sentences = [sentence for sentence in augmented_sentences]\n",
    "\trandom.shuffle(augmented_sentences)\n",
    "\n",
    "\treturn augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GagNpfwiZ3W1"
   },
   "outputs": [],
   "source": [
    "def rs_EDA(sentence, alpha_rs=0.1,  num_aug=9):\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word != \"\"]\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\t# num_new_per_technique = int(num_aug/4) + 1\n",
    "\n",
    "\tn_rs = max(1, int(alpha_rs*num_words))\n",
    "\n",
    "\t# rs\n",
    "\tfor _ in range(num_aug):\n",
    "\t\ta_words = random_swap(words, n_rs)\n",
    "\t\taugmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "\taugmented_sentences = [sentence for sentence in augmented_sentences]\n",
    "\trandom.shuffle(augmented_sentences)\n",
    "\n",
    "\treturn augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Q3Eo97N0kNZL"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/Admin/Desktop/Boostcamp-AI-Tech/dataset/train/train.csv')\n",
    "no_relation_df = train[train['label'] == 'no_relation']\n",
    "other_df = train[train['label'] != 'no_relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "p6dat_uuppxr"
   },
   "outputs": [],
   "source": [
    "def extract_word(x):\n",
    "    return x.split(',')[0].split(':')[-1].strip()[1:-1]\n",
    "\n",
    "def extract_type(x):\n",
    "    return x.split(',')[-1].split(':')[-1].strip()[1:-1]\n",
    "\n",
    "def find_index(sentence, word):\n",
    "    length = len(word)\n",
    "    start_index = sentence.find(word)\n",
    "    end_index = start_index + length -1\n",
    "    return start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "S0UH3jx8pkXn"
   },
   "outputs": [],
   "source": [
    "original_subject = train['subject_entity'].apply(extract_word).tolist()\n",
    "original_object = train['object_entity'].apply(extract_word).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IF97Bzh1qwPe"
   },
   "outputs": [],
   "source": [
    "def ratio_aug(df, eda_list, index_list, subject_word_list, object_word_list):\n",
    "    cnt = len(eda_list[0])\n",
    "    eda_list = np.array(eda_list).reshape(-1)\n",
    "    concat_df = pd.DataFrame()\n",
    "    idx = 0\n",
    "\n",
    "    for i in tqdm(range(len(eda_list))):\n",
    "        eda_idx = index_list[idx]\n",
    "        if subject_word_list[eda_idx] in eda_list[i] and object_word_list[eda_idx] in eda_list[i]:\n",
    "\n",
    "            ss,se = find_index(eda_list[i], subject_word_list[eda_idx])\n",
    "            slabel = extract_type(df['subject_entity'].iloc[eda_idx])\n",
    "            sdict = \"{'word': '\"+subject_word_list[eda_idx]+\"',  \\'start_idx\\': \"+str(ss)+\",  \\'end_idx\\': \"+str(se)+\", 'type': '\"+slabel+\"}\"\n",
    "\n",
    "            os,oe = find_index(eda_list[i], object_word_list[eda_idx])\n",
    "            olabel = extract_type(df['object_entity'].iloc[eda_idx])\n",
    "            odict = \"{'word': '\"+object_word_list[eda_idx]+\"',  \\'start_idx\\': \"+str(os)+\",  \\'end_idx\\': \"+str(oe)+\", 'type': '\"+olabel+\"}\"\n",
    "            data = [{\n",
    "                'id' : i,\n",
    "                'sentence' : eda_list[i],\n",
    "                'subject_entity' : sdict,\n",
    "                'object_entity' : odict,\n",
    "                'label' : df['label'].iloc[eda_idx],\n",
    "                'source' : df['source'].iloc[eda_idx]\n",
    "            }]\n",
    "            new_df = pd.DataFrame(data)\n",
    "            concat_df = pd.concat([concat_df,new_df])\n",
    "\n",
    "        if i % cnt == cnt -1:\n",
    "            idx += 1\n",
    "        \n",
    "    return concat_df\n",
    "\n",
    "def EDA(df):\n",
    "    original_subject = df['subject_entity'].apply(extract_word).tolist()\n",
    "    original_object = df['object_entity'].apply(extract_word).tolist()\n",
    "\n",
    "    # 라벨기준으로 분류한 문장을 담은 리스트\n",
    "    eda_list1 = []\n",
    "    eda_list2 = []\n",
    "    eda_list4 = []\n",
    "    eda_list9 = []\n",
    "\n",
    "    # 위문장의 데이터프레임 인덱스를 담은 리스트\n",
    "    index1 = [] \n",
    "    index2 = []\n",
    "    index4 = []\n",
    "    index9 = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        \"\"\"\n",
    "        증강비율에 따라 리스트 생성후 아래와같은 형식으로 분기\n",
    "        \"\"\"\n",
    "        # 1개 증강\n",
    "        if df['label'].iloc[i] in ['org:top_members/employees', 'per:employee_of']:\n",
    "            # eda_list1.append(rs_EDA(df['sentence'].iloc[i], num_aug=1))\n",
    "            # eda_list1.append(rd_EDA(df['sentence'].iloc[i], num_aug=1))\n",
    "            # index1.append(i)\n",
    "            pass\n",
    "\n",
    "        # 2개 증강\n",
    "        elif df['label'].iloc[i] in ['per:title', 'org:member_of', 'org:alternate_names', 'per:origin', 'org:place_of_headquarters', 'per:date_of_birth', 'per:alternate_names']:\n",
    "            # eda_list2.append(rs_EDA(df['sentence'].iloc[i], num_aug=1))\n",
    "            eda_list2.append(rd_EDA(df['sentence'].iloc[i], num_aug=1))\n",
    "            index2.append(i)\n",
    "\n",
    "        # 4개 증강\n",
    "        elif df['label'].iloc[i] in ['per:spouse', 'per:colleagues', 'per:parents', 'org:founded', 'org:members', 'per:date_of_death', 'org:product', 'per:children']:\n",
    "            # eda_list4.append(rs_EDA(df['sentence'].iloc[i], num_aug=2))\n",
    "            eda_list4.append(rd_EDA(df['sentence'].iloc[i], num_aug=2))\n",
    "            index4.append(i)\n",
    "\n",
    "        # 9개 증강    \n",
    "        else:\n",
    "            eda_list9.append(rs_EDA(df['sentence'].iloc[i], num_aug=9))\n",
    "            eda_list9.append(rd_EDA(df['sentence'].iloc[i], num_aug=9))\n",
    "            index9.append(i)\n",
    "\n",
    "    print(np.array(eda_list1).shape, np.array(eda_list2).shape, np.array(eda_list4).shape, np.array(eda_list9).shape)\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    df1 = pd.DataFrame()\n",
    "    # df1 = ratio_aug(df, eda_list1, index1, original_subject, original_object)\n",
    "    df2 = ratio_aug(df, eda_list2, index2, original_subject, original_object)\n",
    "    df4 = ratio_aug(df, eda_list4, index4, original_subject, original_object)\n",
    "    # df9 = ratio_aug(df, eda_list9, index9, original_subject, original_object)\n",
    "\n",
    "    df_list = [df1, df2, df4]  # [df1, df2, df4, df9]\n",
    "    result_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    result_df = result_df.sample(frac=1)  # 행 랜덤으로 셔플\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYv6DQPFu8Bt",
    "outputId": "9e628380-36ea-409e-eefd-604111845ee6"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1004)\n",
    "random.seed(1004)\n",
    "eda_df = EDA(other_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fpYoopzXwobR"
   },
   "outputs": [],
   "source": [
    "eda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Il3ZBR433nGq"
   },
   "outputs": [],
   "source": [
    "merge_df = pd.concat([train, eda_df])\n",
    "merge_df = merge_df.reset_index(drop=True)\n",
    "merge_df['id'] = list(range(len(merge_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "BCtc2V8G5f3R",
    "outputId": "e2f0a707-a647-4058-f80b-ca8f751ae59a"
   },
   "outputs": [],
   "source": [
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Qaz7Er7mIA5G"
   },
   "outputs": [],
   "source": [
    "# merge_df.to_csv(\"eda_train.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JATqP0zocKdn"
   },
   "source": [
    "# Check EDA_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "HVJPblhq5LdP",
    "outputId": "05f71eb5-35cf-49d3-eea8-e936feb8b369"
   },
   "outputs": [],
   "source": [
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "K3WCZDKs6dJy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "ATU-jU1pXBK6",
    "outputId": "774adad0-2546-46b6-c3d4-5f2c4b910c2e"
   },
   "outputs": [],
   "source": [
    "bs = pd.read_csv('bs_merge_cnen.csv')\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 8))\n",
    "ax = sns.countplot(y='label', data=bs, order = bs['label'].value_counts().index)\n",
    "for i in range(bs['label'].value_counts().shape[0]):\n",
    "    ax.text(x=bs['label'].value_counts()[i], y=i+0.3, s=bs['label'].value_counts()[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvq3WYMJeN9x",
    "outputId": "101f4f4d-716d-413a-ac3c-b191ca5935ae"
   },
   "outputs": [],
   "source": [
    "len(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "UrVWxwMs5Bd1",
    "outputId": "7ca560a3-7f08-4554-de8b-f3e965bdbbe7"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10, 8))\n",
    "ax = sns.countplot(y='label', data=merge_df, order = merge_df['label'].value_counts().index)\n",
    "for i in range(merge_df['label'].value_counts().shape[0]):\n",
    "    ax.text(x=merge_df['label'].value_counts()[i], y=i+0.3, s=merge_df['label'].value_counts()[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lrHREnIk6llN"
   },
   "outputs": [],
   "source": [
    "merge_df['label'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Easy Data Augmentaion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

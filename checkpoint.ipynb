{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5863d1e2-82cc-4caa-8713-71899289a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch, wandb, random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    RobertaConfig,\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    BertTokenizer,\n",
    ")\n",
    "from load_data import *\n",
    "from torch.utils.data import Subset\n",
    "import gc\n",
    "import argparse\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43035d6-ed01-4032-bf2a-139976a1dd5a",
   "metadata": {},
   "source": [
    "# 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509071a-aeaf-4222-808a-180e2d7348ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./results/\"\n",
    "checkpoints = search(path)\n",
    "checkpoints = checkpoints[0] + checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43ebf3cb-02ec-4db4-80b8-bcc279c24528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = []\n",
    "for checkpoint in checkpoints:\n",
    "    model.append(AutoModelForSequenceClassification.from_pretrained(path + '/' + checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48083cf9-3bb2-4396-a230-81d4fe9dc748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 state_dict 가중평균 구하기\n",
    "\n",
    "# 1. 0번째 모델의 state_dict를 모두 0으로 만들기\n",
    "\n",
    "for param_tensor in list(model[0].state_dict())[1:]:\n",
    "    model[0].state_dict()[param_tensor] -= model[0].state_dict()[param_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2deb07b-edc7-442b-b68f-ca712ec69b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 1번째~3번째 모델의 state_dict/3 더하기\n",
    "\n",
    "for i in range(1,len(model)):\n",
    "    for param_tensor in list(model[i].state_dict())[1:]:\n",
    "        model[0].state_dict()[param_tensor] += (model[i].state_dict()[param_tensor]/3).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed128e0f-2792-49c0-8d43-992d04c35582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "roberta.embeddings.position_ids \t tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "         168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "         182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "         210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "         224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "         252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "         266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "         280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "         294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "         308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "         322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "         336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "         364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "         378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "         392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "         406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "         420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "         434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "         448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "         462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "         476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "         490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "         504, 505, 506, 507, 508, 509, 510, 511, 512, 513]])\n",
      "roberta.embeddings.word_embeddings.weight \t tensor([[-0.0250,  0.0095, -0.0174,  ...,  0.0845, -0.0059, -0.0109],\n",
      "        [-0.0220,  0.0202, -0.0613,  ...,  0.0057,  0.0158, -0.0293],\n",
      "        [-0.0492,  0.0008,  0.0385,  ..., -0.0469,  0.0484, -0.0680],\n",
      "        ...,\n",
      "        [-0.0321,  0.0126,  0.0211,  ...,  0.1073,  0.0039, -0.0126],\n",
      "        [-0.0529,  0.0171, -0.0152,  ...,  0.1045,  0.0158, -0.0414],\n",
      "        [-0.0108,  0.0136,  0.0104,  ...,  0.1172, -0.0010, -0.0084]])\n",
      "roberta.embeddings.position_embeddings.weight \t tensor([[ 0.0389, -0.0049, -0.0123,  ...,  0.0079,  0.0034, -0.0174],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0687,  0.0317, -0.0067,  ..., -0.0336, -0.0189,  0.0240],\n",
      "        ...,\n",
      "        [ 0.0013,  0.0204,  0.0062,  ...,  0.0740, -0.0112,  0.0134],\n",
      "        [ 0.0080,  0.0531, -0.0291,  ...,  0.0516, -0.0156,  0.0072],\n",
      "        [-0.0278,  0.0581,  0.0309,  ...,  0.0609, -0.0155,  0.0589]])\n",
      "roberta.embeddings.token_type_embeddings.weight \t tensor([[-2.3997e-04,  1.6247e-04,  2.2109e-05,  ..., -7.0652e-04,\n",
      "          5.7952e-04,  1.0320e-04]])\n",
      "roberta.embeddings.LayerNorm.weight \t tensor([0.7403, 0.8226, 0.8152,  ..., 0.4313, 0.7960, 0.7553])\n",
      "roberta.embeddings.LayerNorm.bias \t tensor([ 0.1219, -0.0527,  0.0493,  ..., -0.1979,  0.0397,  0.0031])\n",
      "roberta.encoder.layer.0.attention.self.query.weight \t tensor([[ 0.0243,  0.0273, -0.0381,  ..., -0.0292,  0.0070, -0.0346],\n",
      "        [-0.0156, -0.0046, -0.0566,  ...,  0.0010,  0.0333,  0.0262],\n",
      "        [-0.0160,  0.0273, -0.0013,  ...,  0.0070, -0.0036, -0.0138],\n",
      "        ...,\n",
      "        [ 0.0219, -0.0333,  0.0354,  ...,  0.0813, -0.0222,  0.0286],\n",
      "        [-0.0239, -0.0291,  0.0121,  ...,  0.0212, -0.0073,  0.0312],\n",
      "        [-0.0023,  0.0375, -0.0300,  ..., -0.0245, -0.0055, -0.0356]])\n",
      "roberta.encoder.layer.0.attention.self.query.bias \t tensor([-0.2462,  0.1444,  0.0766,  ...,  0.0057,  0.0841,  0.0505])\n",
      "roberta.encoder.layer.0.attention.self.key.weight \t tensor([[-4.9521e-02,  1.2626e-02, -4.5593e-03,  ..., -2.4472e-03,\n",
      "         -8.6069e-02, -6.4214e-02],\n",
      "        [-3.8005e-02, -6.1022e-03,  6.6242e-02,  ..., -4.2510e-03,\n",
      "         -6.4923e-03, -4.4234e-02],\n",
      "        [-2.9325e-02, -7.8190e-03, -7.7179e-02,  ...,  3.7088e-02,\n",
      "          1.2941e-02,  1.6378e-02],\n",
      "        ...,\n",
      "        [ 6.5550e-04,  9.4651e-03, -9.3950e-03,  ..., -6.4124e-03,\n",
      "         -2.5505e-02,  6.8671e-02],\n",
      "        [ 1.2259e-02, -3.4590e-04,  6.4740e-03,  ..., -5.4169e-02,\n",
      "         -3.6389e-03, -4.5213e-03],\n",
      "        [ 2.0258e-03, -2.4887e-02,  8.7668e-05,  ..., -3.4551e-02,\n",
      "         -3.4973e-02, -4.2810e-02]])\n",
      "roberta.encoder.layer.0.attention.self.key.bias \t tensor([-4.4006e-05,  4.0376e-06,  1.6025e-04,  ...,  8.8324e-05,\n",
      "         4.5193e-05, -3.0319e-05])\n",
      "roberta.encoder.layer.0.attention.self.value.weight \t tensor([[ 0.0079, -0.0022, -0.0133,  ...,  0.0154, -0.0151,  0.0056],\n",
      "        [-0.0127,  0.0087, -0.0041,  ...,  0.0285, -0.0092, -0.0177],\n",
      "        [-0.0003, -0.0155,  0.0057,  ...,  0.0028,  0.0217,  0.0187],\n",
      "        ...,\n",
      "        [ 0.0223, -0.0176,  0.0046,  ...,  0.0073, -0.0102,  0.0218],\n",
      "        [ 0.0356, -0.0163,  0.0024,  ...,  0.0047,  0.0044, -0.0072],\n",
      "        [ 0.0277, -0.0133,  0.0265,  ...,  0.0340,  0.0430, -0.0104]])\n",
      "roberta.encoder.layer.0.attention.self.value.bias \t tensor([ 0.0794,  0.0552,  0.0135,  ...,  0.0089, -0.0045, -0.0025])\n",
      "roberta.encoder.layer.0.attention.output.dense.weight \t tensor([[ 0.0144,  0.0165, -0.0013,  ..., -0.0262, -0.0271, -0.0056],\n",
      "        [-0.0123, -0.0049, -0.0129,  ...,  0.0060,  0.0085, -0.0274],\n",
      "        [ 0.0027, -0.0031, -0.0135,  ..., -0.0176,  0.0219, -0.0173],\n",
      "        ...,\n",
      "        [ 0.0018,  0.0096, -0.0400,  ..., -0.0150, -0.0128,  0.0686],\n",
      "        [-0.0075, -0.0028, -0.0064,  ..., -0.0074, -0.0055,  0.0172],\n",
      "        [ 0.0085,  0.0306,  0.0093,  ..., -0.0136, -0.0131,  0.0237]])\n",
      "roberta.encoder.layer.0.attention.output.dense.bias \t tensor([ 0.0041, -0.0014,  0.0245,  ...,  0.0201, -0.0105, -0.0023])\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight \t tensor([0.8350, 0.8612, 0.8491,  ..., 0.7295, 0.8414, 0.8182])\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias \t tensor([ 0.0133, -0.0088,  0.0623,  ...,  0.1022, -0.0901, -0.0082])\n",
      "roberta.encoder.layer.0.intermediate.dense.weight \t tensor([[-0.0111,  0.0604, -0.0290,  ..., -0.0196,  0.0080, -0.0531],\n",
      "        [ 0.0268, -0.0214, -0.0432,  ...,  0.0092, -0.0013, -0.0146],\n",
      "        [ 0.0697, -0.0011,  0.0107,  ..., -0.0535,  0.0090, -0.0300],\n",
      "        ...,\n",
      "        [-0.0243,  0.0283, -0.0120,  ...,  0.0129,  0.0038,  0.0258],\n",
      "        [-0.0019,  0.0569, -0.0072,  ..., -0.0516,  0.0066, -0.0122],\n",
      "        [-0.0255, -0.0076,  0.0204,  ...,  0.0350,  0.0784, -0.0182]])\n",
      "roberta.encoder.layer.0.intermediate.dense.bias \t tensor([-0.0623, -0.0442, -0.0453,  ..., -0.0344, -0.0578, -0.0310])\n",
      "roberta.encoder.layer.0.output.dense.weight \t tensor([[ 0.0184, -0.0168,  0.0034,  ..., -0.0122,  0.0071, -0.0014],\n",
      "        [-0.0136, -0.0511, -0.0321,  ..., -0.0170, -0.0051, -0.0327],\n",
      "        [-0.0479, -0.0033,  0.0033,  ..., -0.0049, -0.0220, -0.0479],\n",
      "        ...,\n",
      "        [ 0.0348,  0.0037, -0.0321,  ...,  0.0081, -0.0118,  0.0084],\n",
      "        [-0.0493,  0.0260,  0.0053,  ..., -0.0199, -0.0074,  0.0063],\n",
      "        [-0.0309, -0.0284, -0.0076,  ...,  0.0225,  0.0111,  0.0215]])\n",
      "roberta.encoder.layer.0.output.dense.bias \t tensor([-0.0265,  0.0088,  0.0479,  ...,  0.0098, -0.0264, -0.0193])\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight \t tensor([0.7792, 0.7815, 0.7957,  ..., 0.6720, 0.7793, 0.7649])\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias \t tensor([ 0.0123, -0.0152,  0.0321,  ..., -0.0536, -0.0322, -0.0279])\n",
      "roberta.encoder.layer.1.attention.self.query.weight \t tensor([[ 0.0398, -0.0213, -0.0120,  ...,  0.0017,  0.0114,  0.0389],\n",
      "        [-0.0186,  0.0172, -0.0068,  ...,  0.0259,  0.0285, -0.0144],\n",
      "        [-0.0404, -0.0003, -0.0259,  ...,  0.0422, -0.0301,  0.0160],\n",
      "        ...,\n",
      "        [ 0.0232, -0.0198, -0.0623,  ..., -0.0474,  0.0367,  0.0008],\n",
      "        [-0.0069, -0.0117,  0.0160,  ...,  0.0236,  0.0207,  0.0017],\n",
      "        [ 0.0277,  0.0056,  0.0685,  ..., -0.0044,  0.0315,  0.0251]])\n",
      "roberta.encoder.layer.1.attention.self.query.bias \t tensor([ 0.0079, -0.0321, -0.0245,  ..., -0.0359,  0.0334, -0.0206])\n",
      "roberta.encoder.layer.1.attention.self.key.weight \t tensor([[-0.0308, -0.0146,  0.0574,  ...,  0.0711,  0.0278,  0.0284],\n",
      "        [-0.0127,  0.0122, -0.0024,  ...,  0.0640,  0.0747,  0.0293],\n",
      "        [ 0.0041,  0.0105, -0.0981,  ...,  0.0324, -0.0264,  0.0107],\n",
      "        ...,\n",
      "        [-0.0433, -0.0014,  0.0504,  ...,  0.0550, -0.0441, -0.0089],\n",
      "        [ 0.0054,  0.0317, -0.0029,  ..., -0.0570, -0.0303,  0.0058],\n",
      "        [-0.0073,  0.0191,  0.0455,  ..., -0.0086,  0.0385, -0.0375]])\n",
      "roberta.encoder.layer.1.attention.self.key.bias \t tensor([-3.8857e-05,  7.8426e-06,  1.1843e-05,  ..., -3.2532e-05,\n",
      "         8.9130e-06,  1.2842e-05])\n",
      "roberta.encoder.layer.1.attention.self.value.weight \t tensor([[-6.4766e-04,  3.5872e-02, -2.6757e-03,  ..., -6.7851e-03,\n",
      "         -1.0523e-03,  3.6457e-02],\n",
      "        [ 3.4734e-02,  3.7416e-02, -2.7230e-03,  ..., -7.9624e-03,\n",
      "          2.3391e-02, -2.9690e-02],\n",
      "        [-2.3676e-05, -2.4168e-02, -3.3023e-02,  ..., -2.1806e-02,\n",
      "          2.8322e-02, -2.6623e-02],\n",
      "        ...,\n",
      "        [ 3.3077e-02,  7.0599e-02,  1.6655e-02,  ...,  5.0618e-02,\n",
      "          1.4711e-02,  1.5301e-02],\n",
      "        [-1.1078e-03, -5.8406e-02,  1.8637e-02,  ...,  4.1998e-02,\n",
      "          3.0622e-02, -1.2926e-02],\n",
      "        [-1.0007e-02, -1.0549e-02, -1.9088e-02,  ..., -2.1153e-02,\n",
      "         -2.8474e-02,  2.3266e-02]])\n",
      "roberta.encoder.layer.1.attention.self.value.bias \t tensor([ 0.0121,  0.0119, -0.0001,  ..., -0.0114,  0.0109, -0.0055])\n",
      "roberta.encoder.layer.1.attention.output.dense.weight \t tensor([[-0.0269, -0.0016, -0.0286,  ..., -0.0027, -0.0025,  0.0078],\n",
      "        [ 0.0206,  0.0094, -0.0317,  ...,  0.0138, -0.0199,  0.0046],\n",
      "        [ 0.0208,  0.0041, -0.0030,  ...,  0.0254,  0.0092, -0.0026],\n",
      "        ...,\n",
      "        [-0.0305, -0.0303, -0.0016,  ..., -0.0615, -0.0765, -0.0421],\n",
      "        [ 0.0242, -0.0276, -0.0036,  ..., -0.0368,  0.0267, -0.0199],\n",
      "        [ 0.0031,  0.0137, -0.0311,  ..., -0.0298,  0.0055,  0.0254]])\n",
      "roberta.encoder.layer.1.attention.output.dense.bias \t tensor([ 0.0078, -0.0009,  0.0065,  ...,  0.0236, -0.0055, -0.0293])\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight \t tensor([0.8318, 0.8163, 0.8206,  ..., 0.7990, 0.8310, 0.8192])\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias \t tensor([-0.0036,  0.0048,  0.0741,  ...,  0.0065, -0.1088, -0.0308])\n",
      "roberta.encoder.layer.1.intermediate.dense.weight \t tensor([[ 1.9606e-02,  2.5247e-02,  4.8478e-02,  ...,  2.4220e-02,\n",
      "         -1.5478e-03,  3.7287e-02],\n",
      "        [-4.1537e-02, -3.6115e-02,  3.8413e-02,  ..., -1.1713e-02,\n",
      "         -4.2719e-02, -2.9454e-02],\n",
      "        [ 4.5893e-02, -1.9532e-02, -1.2745e-03,  ...,  7.2109e-02,\n",
      "          3.1571e-02,  1.7108e-02],\n",
      "        ...,\n",
      "        [ 1.8216e-02,  1.0374e-02, -2.1478e-02,  ...,  2.8501e-03,\n",
      "         -9.4554e-04,  2.2490e-02],\n",
      "        [-3.6388e-03,  7.7935e-02, -9.0477e-03,  ...,  1.6455e-02,\n",
      "         -8.4403e-04,  1.0226e-02],\n",
      "        [ 8.0442e-02, -2.9579e-02,  3.2886e-02,  ..., -3.4668e-05,\n",
      "          1.6358e-02,  1.1495e-02]])\n",
      "roberta.encoder.layer.1.intermediate.dense.bias \t tensor([-0.0332, -0.0421, -0.0328,  ..., -0.0155, -0.0376, -0.0341])\n",
      "roberta.encoder.layer.1.output.dense.weight \t tensor([[-0.0333, -0.0171,  0.0303,  ..., -0.0080,  0.0021,  0.0095],\n",
      "        [-0.0336, -0.0281, -0.0193,  ...,  0.0248,  0.0177, -0.0172],\n",
      "        [ 0.0095,  0.0355, -0.0132,  ..., -0.0428,  0.0011,  0.0430],\n",
      "        ...,\n",
      "        [ 0.0443, -0.0657,  0.0205,  ...,  0.0165,  0.0448,  0.0104],\n",
      "        [-0.0102,  0.0005, -0.0356,  ..., -0.0182, -0.0110, -0.0142],\n",
      "        [ 0.0412,  0.0316, -0.0552,  ..., -0.0274,  0.0425,  0.0158]])\n",
      "roberta.encoder.layer.1.output.dense.bias \t tensor([-0.0247,  0.0383,  0.0121,  ..., -0.0336, -0.0044, -0.0251])\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight \t tensor([0.8120, 0.7858, 0.7849,  ..., 0.7350, 0.7812, 0.7753])\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias \t tensor([ 0.0018,  0.0171, -0.0029,  ..., -0.0212,  0.0222,  0.0024])\n",
      "roberta.encoder.layer.2.attention.self.query.weight \t tensor([[ 0.0089, -0.0225, -0.0188,  ..., -0.0191,  0.0214, -0.0042],\n",
      "        [ 0.0075,  0.0485, -0.0294,  ..., -0.0066,  0.0202, -0.0085],\n",
      "        [ 0.0003, -0.0381, -0.0005,  ...,  0.0807, -0.0092,  0.0110],\n",
      "        ...,\n",
      "        [-0.0334, -0.0644,  0.0263,  ...,  0.0281, -0.0222,  0.0056],\n",
      "        [ 0.0073, -0.0392, -0.0200,  ...,  0.0356,  0.0071,  0.0307],\n",
      "        [ 0.0403,  0.0402, -0.0456,  ..., -0.0191,  0.0543,  0.0940]])\n",
      "roberta.encoder.layer.2.attention.self.query.bias \t tensor([ 0.0049,  0.0864, -0.0873,  ..., -0.0197, -0.0308, -0.0128])\n",
      "roberta.encoder.layer.2.attention.self.key.weight \t tensor([[ 0.0087, -0.0244, -0.0276,  ...,  0.0438, -0.0253, -0.0089],\n",
      "        [ 0.0340, -0.0151, -0.0084,  ...,  0.0233, -0.0137, -0.0258],\n",
      "        [-0.0081, -0.0112,  0.0114,  ..., -0.0132, -0.0074, -0.0294],\n",
      "        ...,\n",
      "        [ 0.0292,  0.0385,  0.0737,  ...,  0.0138,  0.0142,  0.0517],\n",
      "        [-0.0103, -0.0083, -0.0211,  ...,  0.0067,  0.0053, -0.0272],\n",
      "        [-0.0125,  0.0004,  0.0192,  ..., -0.0126, -0.0061,  0.0132]])\n",
      "roberta.encoder.layer.2.attention.self.key.bias \t tensor([-1.3837e-05,  1.8347e-05,  4.1726e-05,  ..., -4.7923e-05,\n",
      "        -3.7903e-05,  1.4203e-04])\n",
      "roberta.encoder.layer.2.attention.self.value.weight \t tensor([[ 0.0136, -0.0084,  0.0041,  ...,  0.0160, -0.0205, -0.0036],\n",
      "        [-0.0408,  0.0236, -0.0110,  ...,  0.0301,  0.0215, -0.0369],\n",
      "        [-0.0021,  0.0274, -0.0310,  ...,  0.0517,  0.0368,  0.0186],\n",
      "        ...,\n",
      "        [ 0.0203, -0.0184,  0.0443,  ..., -0.0024,  0.0049, -0.0242],\n",
      "        [-0.0140,  0.0449,  0.0077,  ..., -0.0131,  0.0325, -0.0034],\n",
      "        [-0.0165,  0.0078,  0.0339,  ..., -0.0108, -0.0123,  0.0048]])\n",
      "roberta.encoder.layer.2.attention.self.value.bias \t tensor([ 0.0154,  0.0008,  0.0077,  ...,  0.0005, -0.0064,  0.0140])\n",
      "roberta.encoder.layer.2.attention.output.dense.weight \t tensor([[-0.0012,  0.0241,  0.0065,  ..., -0.0321,  0.0117,  0.0068],\n",
      "        [-0.0025, -0.0283,  0.0221,  ..., -0.0080, -0.0122,  0.0204],\n",
      "        [-0.0136, -0.0159,  0.0288,  ..., -0.0267,  0.0209, -0.0457],\n",
      "        ...,\n",
      "        [-0.0268, -0.0187, -0.0219,  ...,  0.0001,  0.0316, -0.0251],\n",
      "        [ 0.0139, -0.0483,  0.0013,  ..., -0.0083, -0.0096, -0.0068],\n",
      "        [ 0.0080,  0.0078, -0.0349,  ...,  0.0255,  0.0161,  0.0245]])\n",
      "roberta.encoder.layer.2.attention.output.dense.bias \t tensor([-0.0013,  0.0257,  0.0004,  ..., -0.0261, -0.0235,  0.0074])\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight \t tensor([0.8472, 0.7887, 0.7972,  ..., 0.7770, 0.8050, 0.7952])\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias \t tensor([-0.0232, -0.0002,  0.0421,  ...,  0.0406, -0.1308, -0.0283])\n",
      "roberta.encoder.layer.2.intermediate.dense.weight \t tensor([[ 0.0196,  0.0292, -0.0276,  ...,  0.0293,  0.0182,  0.0001],\n",
      "        [-0.0218, -0.0235, -0.0074,  ..., -0.0202,  0.0175,  0.0172],\n",
      "        [-0.0063,  0.0019, -0.0111,  ..., -0.0046, -0.0095, -0.0885],\n",
      "        ...,\n",
      "        [-0.0060, -0.0022, -0.0159,  ..., -0.0312, -0.0160,  0.0605],\n",
      "        [ 0.0002,  0.0079,  0.0260,  ..., -0.0701,  0.0031,  0.0222],\n",
      "        [-0.0286, -0.0160, -0.0287,  ...,  0.0104,  0.0122, -0.0096]])\n",
      "roberta.encoder.layer.2.intermediate.dense.bias \t tensor([-0.0360, -0.0416, -0.0209,  ..., -0.0186, -0.0343, -0.0349])\n",
      "roberta.encoder.layer.2.output.dense.weight \t tensor([[ 2.1467e-02, -3.3498e-02, -2.9973e-02,  ...,  2.0876e-02,\n",
      "          5.1059e-03, -1.9344e-02],\n",
      "        [-3.8414e-02, -1.9254e-02, -1.2072e-02,  ...,  1.7554e-02,\n",
      "         -3.2843e-02, -4.3078e-02],\n",
      "        [-6.9323e-02,  9.5727e-03, -3.6176e-04,  ...,  3.0934e-02,\n",
      "         -4.8608e-02,  1.1914e-02],\n",
      "        ...,\n",
      "        [-5.1179e-02, -3.7458e-02, -1.9298e-03,  ...,  1.5773e-02,\n",
      "         -3.0882e-02, -3.7053e-02],\n",
      "        [-1.8107e-02,  2.1363e-02, -2.3901e-02,  ..., -1.1809e-02,\n",
      "          8.9231e-03, -9.0045e-03],\n",
      "        [ 8.3494e-03,  1.3773e-02, -6.6601e-03,  ...,  3.2241e-02,\n",
      "          3.6283e-05,  1.0530e-02]])\n",
      "roberta.encoder.layer.2.output.dense.bias \t tensor([ 0.0022,  0.0341,  0.0139,  ..., -0.0124, -0.0020, -0.0038])\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight \t tensor([0.8544, 0.7813, 0.8007,  ..., 0.7146, 0.7781, 0.7677])\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias \t tensor([ 0.0052,  0.0173,  0.0082,  ..., -0.0160,  0.0417,  0.0467])\n",
      "roberta.encoder.layer.3.attention.self.query.weight \t tensor([[-0.0173,  0.0022, -0.0042,  ...,  0.0171, -0.0615, -0.0618],\n",
      "        [ 0.0130, -0.0588,  0.0136,  ..., -0.0032, -0.0019, -0.0357],\n",
      "        [ 0.0233, -0.0268,  0.1051,  ..., -0.0440, -0.0290, -0.0192],\n",
      "        ...,\n",
      "        [ 0.0139,  0.0424, -0.0039,  ..., -0.0200, -0.0151, -0.0119],\n",
      "        [-0.0161, -0.0452, -0.0197,  ..., -0.0132,  0.0277,  0.0017],\n",
      "        [-0.0411, -0.0233,  0.0367,  ...,  0.0028, -0.0323, -0.0064]])\n",
      "roberta.encoder.layer.3.attention.self.query.bias \t tensor([-0.0337, -0.0024,  0.0034,  ..., -0.0866,  0.0555,  0.0038])\n",
      "roberta.encoder.layer.3.attention.self.key.weight \t tensor([[-0.0234,  0.0073, -0.0275,  ...,  0.0008, -0.0518, -0.0230],\n",
      "        [ 0.0025, -0.0379,  0.0100,  ...,  0.0635, -0.0287, -0.0247],\n",
      "        [ 0.0299,  0.0147,  0.0812,  ...,  0.0275, -0.0068, -0.0171],\n",
      "        ...,\n",
      "        [ 0.0335, -0.0109, -0.0045,  ...,  0.0408,  0.0174,  0.0010],\n",
      "        [-0.0098,  0.0337, -0.0190,  ...,  0.0156,  0.0196, -0.0325],\n",
      "        [ 0.0495,  0.0229, -0.0265,  ..., -0.0263,  0.0168, -0.0341]])\n",
      "roberta.encoder.layer.3.attention.self.key.bias \t tensor([ 3.2226e-05,  1.2178e-05, -2.9304e-04,  ..., -4.4692e-05,\n",
      "        -2.2981e-05, -2.5742e-05])\n",
      "roberta.encoder.layer.3.attention.self.value.weight \t tensor([[-0.0372, -0.0132,  0.0203,  ...,  0.0176, -0.0101, -0.0110],\n",
      "        [-0.0417,  0.0181,  0.0350,  ..., -0.0062, -0.0066, -0.0133],\n",
      "        [ 0.0150, -0.0109,  0.0181,  ...,  0.0153, -0.0055, -0.0110],\n",
      "        ...,\n",
      "        [ 0.0223, -0.0062,  0.0532,  ...,  0.0045,  0.0115, -0.0079],\n",
      "        [-0.0136, -0.0344, -0.0111,  ..., -0.0414,  0.0227,  0.0233],\n",
      "        [-0.0042,  0.0332, -0.0419,  ..., -0.0367,  0.0159,  0.0016]])\n",
      "roberta.encoder.layer.3.attention.self.value.bias \t tensor([-5.7701e-03,  6.8269e-05,  5.6932e-03,  ...,  2.9149e-03,\n",
      "        -1.7613e-03,  1.8786e-03])\n",
      "roberta.encoder.layer.3.attention.output.dense.weight \t tensor([[-0.0272,  0.0337,  0.0226,  ..., -0.0077, -0.0029, -0.0063],\n",
      "        [ 0.0045,  0.0027, -0.0185,  ...,  0.0320,  0.0093, -0.0276],\n",
      "        [-0.0248, -0.0083,  0.0202,  ..., -0.0492,  0.0416,  0.0343],\n",
      "        ...,\n",
      "        [-0.0095, -0.0066,  0.0242,  ..., -0.0193,  0.0085,  0.0032],\n",
      "        [-0.0065,  0.0302, -0.0113,  ..., -0.0230,  0.0302, -0.0079],\n",
      "        [-0.0090,  0.0262,  0.0120,  ...,  0.0053,  0.0004, -0.0229]])\n",
      "roberta.encoder.layer.3.attention.output.dense.bias \t tensor([-0.0053,  0.0018, -0.0096,  ...,  0.0397,  0.0441,  0.0030])\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight \t tensor([0.9030, 0.7717, 0.7931,  ..., 0.7526, 0.7930, 0.7720])\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias \t tensor([-0.0289,  0.0103,  0.0469,  ...,  0.0479, -0.0798,  0.0296])\n",
      "roberta.encoder.layer.3.intermediate.dense.weight \t tensor([[ 0.0257, -0.0029, -0.0257,  ...,  0.0022, -0.0381, -0.0312],\n",
      "        [-0.0106,  0.0667, -0.0138,  ..., -0.0151,  0.0585, -0.0213],\n",
      "        [ 0.0146, -0.0703, -0.0236,  ..., -0.0241,  0.0443,  0.0256],\n",
      "        ...,\n",
      "        [-0.0262,  0.0187,  0.0094,  ..., -0.0049,  0.0082,  0.0170],\n",
      "        [-0.0245, -0.0080,  0.0235,  ..., -0.0015, -0.0285, -0.0026],\n",
      "        [-0.0167,  0.0086,  0.0094,  ..., -0.0146, -0.0302,  0.0287]])\n",
      "roberta.encoder.layer.3.intermediate.dense.bias \t tensor([-0.0339, -0.0203, -0.0189,  ...,  0.0019, -0.0361, -0.0343])\n",
      "roberta.encoder.layer.3.output.dense.weight \t tensor([[ 0.0296, -0.0217,  0.0108,  ..., -0.0240, -0.0194,  0.0139],\n",
      "        [-0.0061,  0.0308,  0.0337,  ...,  0.0112, -0.0277,  0.0193],\n",
      "        [ 0.0112,  0.0039,  0.0133,  ..., -0.0623, -0.0210, -0.0319],\n",
      "        ...,\n",
      "        [ 0.0313,  0.0306,  0.0138,  ..., -0.0013, -0.0141, -0.0533],\n",
      "        [-0.0321, -0.0066,  0.0115,  ...,  0.0170, -0.0351,  0.0365],\n",
      "        [-0.0309, -0.0994,  0.0009,  ..., -0.0261, -0.0190, -0.0495]])\n",
      "roberta.encoder.layer.3.output.dense.bias \t tensor([-0.0489,  0.0185,  0.0188,  ..., -0.0214, -0.0102,  0.0039])\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight \t tensor([0.8562, 0.8108, 0.8326,  ..., 0.7195, 0.7912, 0.7893])\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias \t tensor([-0.0009, -0.0082,  0.0352,  ...,  0.0052,  0.0131,  0.0045])\n",
      "roberta.encoder.layer.4.attention.self.query.weight \t tensor([[-0.0154,  0.0344,  0.0311,  ...,  0.0191, -0.0241,  0.0310],\n",
      "        [ 0.0207, -0.0036,  0.0645,  ...,  0.0651,  0.0306, -0.0577],\n",
      "        [ 0.0357,  0.0048, -0.0271,  ..., -0.0216,  0.0360, -0.0089],\n",
      "        ...,\n",
      "        [-0.0518, -0.0497, -0.0487,  ...,  0.0462,  0.0536,  0.0515],\n",
      "        [ 0.0355, -0.0171, -0.0016,  ..., -0.0350, -0.0235,  0.0019],\n",
      "        [ 0.0141, -0.0201, -0.0147,  ...,  0.0257,  0.0479,  0.0395]])\n",
      "roberta.encoder.layer.4.attention.self.query.bias \t tensor([ 0.0349,  0.0012,  0.0033,  ..., -0.1318, -0.0318,  0.0379])\n",
      "roberta.encoder.layer.4.attention.self.key.weight \t tensor([[ 0.0048,  0.0004,  0.0060,  ..., -0.0305, -0.0159, -0.0615],\n",
      "        [-0.0132, -0.0207,  0.0383,  ..., -0.0231,  0.0320,  0.0698],\n",
      "        [-0.0660,  0.0040, -0.0258,  ...,  0.0356,  0.0081, -0.0768],\n",
      "        ...,\n",
      "        [ 0.0172, -0.0024, -0.0132,  ..., -0.0058,  0.0518, -0.0081],\n",
      "        [-0.0329, -0.0041, -0.0225,  ..., -0.0159,  0.0273,  0.0201],\n",
      "        [ 0.0489,  0.0137, -0.0185,  ...,  0.0248,  0.0340,  0.0166]])\n",
      "roberta.encoder.layer.4.attention.self.key.bias \t tensor([ 8.4876e-05,  5.2989e-05, -3.4521e-05,  ..., -4.1145e-04,\n",
      "         1.7515e-05,  1.8814e-04])\n",
      "roberta.encoder.layer.4.attention.self.value.weight \t tensor([[-0.0215, -0.0082, -0.0063,  ..., -0.0115, -0.0420, -0.0261],\n",
      "        [ 0.0081,  0.0410,  0.0158,  ...,  0.0047, -0.0677,  0.0574],\n",
      "        [ 0.0101,  0.0107, -0.0160,  ...,  0.0265,  0.0397,  0.0186],\n",
      "        ...,\n",
      "        [-0.0005,  0.0237, -0.0003,  ..., -0.0204, -0.0096,  0.0077],\n",
      "        [ 0.0096,  0.0002, -0.0154,  ..., -0.0158, -0.0357, -0.0050],\n",
      "        [ 0.0179,  0.0072,  0.0238,  ..., -0.0090,  0.0385,  0.0252]])\n",
      "roberta.encoder.layer.4.attention.self.value.bias \t tensor([-0.0108, -0.0048, -0.0008,  ...,  0.0059, -0.0132, -0.0255])\n",
      "roberta.encoder.layer.4.attention.output.dense.weight \t tensor([[-0.0192, -0.0021, -0.0003,  ..., -0.0110, -0.0107,  0.0153],\n",
      "        [ 0.0183, -0.0241,  0.0043,  ..., -0.0213,  0.0172, -0.0245],\n",
      "        [ 0.0265, -0.0316, -0.0104,  ..., -0.0080, -0.0108, -0.0134],\n",
      "        ...,\n",
      "        [ 0.0088,  0.0176, -0.0222,  ...,  0.0120,  0.0465, -0.0483],\n",
      "        [ 0.0275,  0.0303, -0.0135,  ..., -0.0280, -0.0053, -0.0358],\n",
      "        [ 0.0059, -0.0081,  0.0167,  ...,  0.0316, -0.0373,  0.0209]])\n",
      "roberta.encoder.layer.4.attention.output.dense.bias \t tensor([-0.0348,  0.0099,  0.0031,  ..., -0.0246,  0.0549,  0.0782])\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight \t tensor([0.7346, 0.7865, 0.8115,  ..., 0.7358, 0.8078, 0.7804])\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias \t tensor([-0.0370, -0.0123,  0.1180,  ...,  0.0409, -0.1063, -0.0209])\n",
      "roberta.encoder.layer.4.intermediate.dense.weight \t tensor([[-0.0123,  0.0223,  0.0153,  ...,  0.0223,  0.0242, -0.0396],\n",
      "        [-0.0222, -0.0215,  0.0419,  ...,  0.0758,  0.0453,  0.0351],\n",
      "        [-0.0098,  0.0144, -0.0306,  ..., -0.0584,  0.0089, -0.0015],\n",
      "        ...,\n",
      "        [-0.0183,  0.0450, -0.0139,  ..., -0.0299,  0.0393, -0.0715],\n",
      "        [ 0.0052,  0.0283, -0.0339,  ...,  0.0064,  0.0578,  0.0331],\n",
      "        [-0.0121, -0.0159, -0.0286,  ...,  0.0357, -0.0611, -0.0080]])\n",
      "roberta.encoder.layer.4.intermediate.dense.bias \t tensor([-0.0448, -0.0262, -0.0257,  ..., -0.0253, -0.0328, -0.0255])\n",
      "roberta.encoder.layer.4.output.dense.weight \t tensor([[-0.0083,  0.0169,  0.0379,  ..., -0.0103, -0.0026,  0.0054],\n",
      "        [-0.0425, -0.0336,  0.0194,  ...,  0.0236,  0.0203,  0.0010],\n",
      "        [-0.0228,  0.0127,  0.0349,  ..., -0.0260, -0.0255,  0.0117],\n",
      "        ...,\n",
      "        [-0.0043,  0.0117,  0.0471,  ...,  0.0112, -0.0006,  0.0115],\n",
      "        [-0.0166, -0.0459, -0.0407,  ...,  0.0744,  0.0263,  0.0073],\n",
      "        [-0.0037,  0.0543,  0.0044,  ..., -0.0078, -0.0009, -0.0357]])\n",
      "roberta.encoder.layer.4.output.dense.bias \t tensor([ 0.0089,  0.0292,  0.0204,  ..., -0.0239, -0.0066, -0.0122])\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight \t tensor([0.8013, 0.7787, 0.7961,  ..., 0.7371, 0.7600, 0.7720])\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias \t tensor([ 0.0337, -0.0145, -0.0240,  ..., -0.0548,  0.0913,  0.0230])\n",
      "roberta.encoder.layer.5.attention.self.query.weight \t tensor([[ 0.0545,  0.0174, -0.0208,  ..., -0.0227,  0.0686, -0.0060],\n",
      "        [-0.0039,  0.0332, -0.0168,  ...,  0.0361, -0.0416,  0.0407],\n",
      "        [-0.0548,  0.0389,  0.0100,  ..., -0.0303, -0.0034,  0.0127],\n",
      "        ...,\n",
      "        [ 0.0252,  0.0169, -0.0075,  ...,  0.0471,  0.0411,  0.0252],\n",
      "        [ 0.0128, -0.0025,  0.0029,  ..., -0.0277,  0.0309,  0.0351],\n",
      "        [-0.0079,  0.0205,  0.0049,  ...,  0.0032,  0.0160,  0.0165]])\n",
      "roberta.encoder.layer.5.attention.self.query.bias \t tensor([ 0.1640, -0.1445,  0.0510,  ..., -0.0636, -0.0437, -0.0008])\n",
      "roberta.encoder.layer.5.attention.self.key.weight \t tensor([[-0.0060,  0.0060, -0.0152,  ...,  0.0135,  0.0131, -0.0292],\n",
      "        [-0.0121,  0.0228,  0.0439,  ...,  0.0438,  0.0230, -0.0169],\n",
      "        [-0.0230, -0.0082, -0.0027,  ...,  0.0117, -0.0048,  0.0051],\n",
      "        ...,\n",
      "        [ 0.0236,  0.0294, -0.0796,  ...,  0.0359,  0.0168,  0.0131],\n",
      "        [-0.0327,  0.0048, -0.0483,  ..., -0.0128,  0.0135,  0.0027],\n",
      "        [-0.0040,  0.0006, -0.0023,  ..., -0.0160, -0.0427, -0.0265]])\n",
      "roberta.encoder.layer.5.attention.self.key.bias \t tensor([-5.7090e-05,  7.0896e-05,  7.9787e-05,  ..., -2.6860e-04,\n",
      "        -2.5402e-04,  6.5507e-05])\n",
      "roberta.encoder.layer.5.attention.self.value.weight \t tensor([[-0.0135,  0.0288,  0.0029,  ...,  0.0057, -0.0054, -0.0253],\n",
      "        [-0.0241,  0.0020,  0.0191,  ...,  0.0452, -0.0094,  0.0566],\n",
      "        [ 0.0085, -0.0050,  0.0023,  ...,  0.0424, -0.0508, -0.0090],\n",
      "        ...,\n",
      "        [-0.0017, -0.0328,  0.0255,  ...,  0.0313, -0.0038, -0.0001],\n",
      "        [ 0.0069,  0.0487, -0.0121,  ..., -0.0461, -0.0178,  0.0093],\n",
      "        [-0.0070,  0.0108,  0.0094,  ..., -0.0079, -0.0027, -0.0086]])\n",
      "roberta.encoder.layer.5.attention.self.value.bias \t tensor([-0.0116, -0.0015,  0.0010,  ..., -0.0041, -0.0095,  0.0029])\n",
      "roberta.encoder.layer.5.attention.output.dense.weight \t tensor([[ 0.0012, -0.0100,  0.0092,  ..., -0.0093,  0.0124, -0.0409],\n",
      "        [-0.0192,  0.0123, -0.0117,  ..., -0.0020,  0.0111, -0.0145],\n",
      "        [ 0.0073, -0.0340,  0.0221,  ...,  0.0161, -0.0042, -0.0260],\n",
      "        ...,\n",
      "        [ 0.0050, -0.0491, -0.0126,  ..., -0.0109,  0.0328,  0.0239],\n",
      "        [ 0.0105, -0.0096,  0.0325,  ..., -0.0277,  0.0254,  0.0052],\n",
      "        [ 0.0064, -0.0063,  0.0234,  ...,  0.0347, -0.0269,  0.0221]])\n",
      "roberta.encoder.layer.5.attention.output.dense.bias \t tensor([-0.0030,  0.0145,  0.0142,  ..., -0.0263,  0.0027,  0.0311])\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight \t tensor([0.8253, 0.7797, 0.7936,  ..., 0.7303, 0.7789, 0.7710])\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias \t tensor([-0.0187, -0.0588,  0.1212,  ..., -0.0244, -0.0620,  0.0355])\n",
      "roberta.encoder.layer.5.intermediate.dense.weight \t tensor([[ 5.6518e-03,  3.8694e-02, -3.9332e-02,  ..., -7.8251e-05,\n",
      "         -1.0365e-03, -3.2365e-04],\n",
      "        [-1.0970e-02, -4.4623e-02, -6.1341e-03,  ...,  4.5680e-02,\n",
      "          3.0487e-02,  2.0479e-02],\n",
      "        [-2.6930e-03,  6.1504e-02,  7.8709e-03,  ..., -2.6503e-03,\n",
      "         -3.1245e-02,  3.5841e-02],\n",
      "        ...,\n",
      "        [-2.7873e-03, -2.2726e-04, -3.5986e-02,  ..., -4.2492e-02,\n",
      "         -4.5794e-03,  4.2995e-02],\n",
      "        [-3.8175e-02,  1.3961e-02, -2.2243e-02,  ...,  1.6399e-02,\n",
      "         -2.1782e-02,  6.8341e-02],\n",
      "        [ 1.9772e-02,  3.6132e-02,  1.6048e-02,  ..., -6.9221e-03,\n",
      "          5.3769e-03,  1.1244e-02]])\n",
      "roberta.encoder.layer.5.intermediate.dense.bias \t tensor([-0.0301, -0.0292, -0.0203,  ..., -0.0193, -0.0265, -0.0128])\n",
      "roberta.encoder.layer.5.output.dense.weight \t tensor([[ 0.0216,  0.0042, -0.0307,  ...,  0.0052,  0.0237, -0.0119],\n",
      "        [ 0.0211, -0.0103,  0.0076,  ..., -0.0019, -0.0287, -0.0139],\n",
      "        [ 0.0365,  0.0085,  0.0059,  ...,  0.0091, -0.0261, -0.0217],\n",
      "        ...,\n",
      "        [-0.0264, -0.0103, -0.0080,  ..., -0.0213,  0.0065, -0.0266],\n",
      "        [ 0.0216, -0.0208,  0.0335,  ..., -0.0416,  0.0091, -0.0037],\n",
      "        [ 0.0090, -0.0301,  0.0243,  ...,  0.0017, -0.0033, -0.0437]])\n",
      "roberta.encoder.layer.5.output.dense.bias \t tensor([ 0.0022,  0.0197,  0.0044,  ..., -0.0411, -0.0059,  0.0093])\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight \t tensor([0.8295, 0.7553, 0.7857,  ..., 0.7170, 0.7651, 0.7671])\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias \t tensor([ 0.0390,  0.0175, -0.0486,  ...,  0.0076,  0.0484,  0.0082])\n",
      "roberta.encoder.layer.6.attention.self.query.weight \t tensor([[ 0.0091, -0.0245, -0.0640,  ...,  0.0247,  0.0390,  0.0149],\n",
      "        [ 0.0069,  0.0248,  0.0183,  ..., -0.0092,  0.0010, -0.0079],\n",
      "        [-0.0352, -0.0071,  0.0075,  ...,  0.0170, -0.0718,  0.0591],\n",
      "        ...,\n",
      "        [ 0.0105,  0.0180,  0.0162,  ...,  0.0267,  0.0445,  0.0198],\n",
      "        [-0.0387, -0.0017, -0.0161,  ...,  0.1175,  0.0051, -0.0151],\n",
      "        [-0.0031,  0.0118, -0.0054,  ...,  0.0159, -0.0074, -0.0320]])\n",
      "roberta.encoder.layer.6.attention.self.query.bias \t tensor([ 0.0516,  0.0391,  0.0204,  ..., -0.0678,  0.0480,  0.0126])\n",
      "roberta.encoder.layer.6.attention.self.key.weight \t tensor([[ 0.0196, -0.0375,  0.0065,  ..., -0.0621, -0.0256,  0.0154],\n",
      "        [-0.0170,  0.0570, -0.0139,  ..., -0.0067,  0.0220,  0.0018],\n",
      "        [ 0.0573,  0.0662, -0.0074,  ...,  0.0179,  0.0317,  0.0344],\n",
      "        ...,\n",
      "        [ 0.0250, -0.0175, -0.0224,  ..., -0.0370, -0.0322, -0.0004],\n",
      "        [-0.0292,  0.0599, -0.0700,  ..., -0.0035, -0.0123,  0.0042],\n",
      "        [-0.0050, -0.0436,  0.0675,  ..., -0.0373, -0.0541,  0.0614]])\n",
      "roberta.encoder.layer.6.attention.self.key.bias \t tensor([-4.1111e-05,  9.7501e-05, -3.2944e-06,  ...,  1.6659e-04,\n",
      "        -2.2679e-04,  1.9291e-04])\n",
      "roberta.encoder.layer.6.attention.self.value.weight \t tensor([[ 0.0261,  0.0131, -0.0135,  ..., -0.0174, -0.0141, -0.0554],\n",
      "        [ 0.0438,  0.0385, -0.0296,  ...,  0.0208, -0.0540, -0.0110],\n",
      "        [-0.0117, -0.0116,  0.0085,  ..., -0.0059,  0.0102, -0.0496],\n",
      "        ...,\n",
      "        [-0.0127, -0.0146,  0.0050,  ..., -0.0170, -0.0640, -0.0269],\n",
      "        [ 0.0204,  0.0361,  0.0691,  ..., -0.0066, -0.0403, -0.0388],\n",
      "        [-0.0026, -0.0184,  0.0109,  ..., -0.0147, -0.0279, -0.0049]])\n",
      "roberta.encoder.layer.6.attention.self.value.bias \t tensor([-0.0112, -0.0031,  0.0134,  ...,  0.0091, -0.0025, -0.0026])\n",
      "roberta.encoder.layer.6.attention.output.dense.weight \t tensor([[-0.0095,  0.0100,  0.0027,  ..., -0.0022,  0.0148, -0.0028],\n",
      "        [ 0.0091, -0.0241, -0.0131,  ...,  0.0059,  0.0115,  0.0343],\n",
      "        [ 0.0009,  0.0199,  0.0048,  ..., -0.0027,  0.0158,  0.0011],\n",
      "        ...,\n",
      "        [-0.0122, -0.0241,  0.0108,  ..., -0.0260, -0.0050, -0.0119],\n",
      "        [-0.0434,  0.0137,  0.0195,  ..., -0.0296,  0.0154,  0.0138],\n",
      "        [-0.0041, -0.0126,  0.0134,  ..., -0.0263, -0.0057, -0.0283]])\n",
      "roberta.encoder.layer.6.attention.output.dense.bias \t tensor([-0.0092,  0.0537,  0.0069,  ...,  0.0065,  0.0279,  0.0052])\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight \t tensor([0.8448, 0.7669, 0.7840,  ..., 0.7154, 0.7629, 0.7398])\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias \t tensor([ 0.0055, -0.1000,  0.0971,  ..., -0.0603, -0.0658,  0.0362])\n",
      "roberta.encoder.layer.6.intermediate.dense.weight \t tensor([[-0.0289,  0.0306,  0.0296,  ...,  0.0079,  0.0025, -0.0142],\n",
      "        [ 0.0013,  0.0126, -0.0245,  ..., -0.0066, -0.0102, -0.0193],\n",
      "        [ 0.0298,  0.0557,  0.0023,  ..., -0.0305,  0.0335, -0.0322],\n",
      "        ...,\n",
      "        [-0.0242, -0.0263, -0.0276,  ..., -0.0020,  0.0487, -0.0097],\n",
      "        [ 0.0205,  0.0535, -0.0204,  ..., -0.0393,  0.0452, -0.0172],\n",
      "        [-0.0051, -0.0157,  0.0190,  ..., -0.0169,  0.0507, -0.0248]])\n",
      "roberta.encoder.layer.6.intermediate.dense.bias \t tensor([-0.0268, -0.0125, -0.0316,  ..., -0.0281, -0.0211, -0.0317])\n",
      "roberta.encoder.layer.6.output.dense.weight \t tensor([[-2.2354e-02,  2.8294e-02, -2.7805e-02,  ..., -1.6083e-02,\n",
      "          1.0152e-03,  1.3415e-02],\n",
      "        [-2.1838e-02, -3.4582e-02,  3.6755e-02,  ...,  5.2670e-03,\n",
      "          5.6438e-03,  1.8298e-02],\n",
      "        [-1.6151e-02,  2.1446e-02,  2.6029e-02,  ...,  8.2940e-03,\n",
      "         -2.6831e-02, -2.9544e-02],\n",
      "        ...,\n",
      "        [ 7.1455e-02, -2.1464e-02,  7.0672e-05,  ..., -4.0489e-02,\n",
      "          8.8828e-03, -6.1817e-02],\n",
      "        [-1.1148e-02,  3.9615e-03, -3.3505e-03,  ..., -8.4476e-03,\n",
      "         -1.4805e-02, -3.7454e-02],\n",
      "        [-1.9244e-02,  2.0215e-02,  4.7183e-02,  ...,  2.1503e-02,\n",
      "          8.0422e-03, -2.1067e-02]])\n",
      "roberta.encoder.layer.6.output.dense.bias \t tensor([-0.0122,  0.0040, -0.0015,  ..., -0.0313, -0.0013,  0.0095])\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight \t tensor([0.8857, 0.7819, 0.8215,  ..., 0.7420, 0.8014, 0.7969])\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias \t tensor([ 0.0086,  0.0476, -0.0411,  ...,  0.0294,  0.0386, -0.0133])\n",
      "roberta.encoder.layer.7.attention.self.query.weight \t tensor([[ 0.0193, -0.0114, -0.0068,  ..., -0.0408, -0.0502,  0.0339],\n",
      "        [-0.0079,  0.0260,  0.0066,  ...,  0.0667,  0.0021, -0.0083],\n",
      "        [ 0.0209, -0.0105,  0.0214,  ...,  0.0128,  0.0056, -0.0153],\n",
      "        ...,\n",
      "        [-0.0245, -0.0348, -0.0298,  ..., -0.0380,  0.0107, -0.0266],\n",
      "        [-0.0017, -0.0009,  0.0086,  ..., -0.0375,  0.0205, -0.0378],\n",
      "        [ 0.0178,  0.0038, -0.0207,  ...,  0.0075,  0.0308,  0.0059]])\n",
      "roberta.encoder.layer.7.attention.self.query.bias \t tensor([ 0.0026, -0.0646,  0.0180,  ...,  0.0778, -0.0087, -0.0142])\n",
      "roberta.encoder.layer.7.attention.self.key.weight \t tensor([[-0.0475,  0.0749, -0.0064,  ..., -0.0154,  0.0661, -0.0353],\n",
      "        [-0.0110,  0.0088,  0.0232,  ..., -0.0325, -0.0399,  0.0080],\n",
      "        [ 0.0112, -0.0003, -0.0062,  ...,  0.0211,  0.0033,  0.0461],\n",
      "        ...,\n",
      "        [ 0.0555, -0.0352,  0.0601,  ...,  0.0152,  0.0568, -0.0271],\n",
      "        [ 0.0292,  0.0043,  0.0339,  ..., -0.0028, -0.0092,  0.0201],\n",
      "        [-0.0219,  0.0179,  0.0587,  ..., -0.0083, -0.0235, -0.0005]])\n",
      "roberta.encoder.layer.7.attention.self.key.bias \t tensor([3.5490e-05, 9.3564e-05, 2.3822e-05,  ..., 1.3076e-04, 2.1556e-05,\n",
      "        2.7765e-05])\n",
      "roberta.encoder.layer.7.attention.self.value.weight \t tensor([[ 0.0092, -0.0314,  0.0109,  ..., -0.0123,  0.0206, -0.0061],\n",
      "        [ 0.0278,  0.0046,  0.0476,  ...,  0.0395, -0.0297, -0.0342],\n",
      "        [ 0.0284, -0.0274,  0.0222,  ..., -0.0068,  0.0701, -0.0112],\n",
      "        ...,\n",
      "        [-0.0312, -0.0334, -0.0220,  ...,  0.0127,  0.0398, -0.0272],\n",
      "        [-0.0415,  0.0320,  0.0092,  ..., -0.0157,  0.0286,  0.0113],\n",
      "        [-0.0410,  0.0323, -0.0045,  ...,  0.0413, -0.0089,  0.0198]])\n",
      "roberta.encoder.layer.7.attention.self.value.bias \t tensor([-0.0096,  0.0098,  0.0024,  ...,  0.0041,  0.0086, -0.0068])\n",
      "roberta.encoder.layer.7.attention.output.dense.weight \t tensor([[-0.0210, -0.0058, -0.0019,  ...,  0.0217,  0.0149,  0.0053],\n",
      "        [-0.0077,  0.0519,  0.0013,  ...,  0.0137,  0.0398, -0.0505],\n",
      "        [-0.0204, -0.0518,  0.0021,  ..., -0.0188,  0.0268,  0.0129],\n",
      "        ...,\n",
      "        [ 0.0118,  0.0035, -0.0469,  ...,  0.0108, -0.0132, -0.0134],\n",
      "        [-0.0079,  0.0411, -0.0369,  ..., -0.0093, -0.0124, -0.0250],\n",
      "        [-0.0106,  0.0007, -0.0040,  ...,  0.0289, -0.0197, -0.0377]])\n",
      "roberta.encoder.layer.7.attention.output.dense.bias \t tensor([-0.0075,  0.0506,  0.0144,  ...,  0.0128,  0.0534, -0.0003])\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight \t tensor([0.7527, 0.7643, 0.8065,  ..., 0.6907, 0.7577, 0.7483])\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias \t tensor([-0.0108, -0.0830,  0.1004,  ..., -0.0524, -0.0576,  0.0171])\n",
      "roberta.encoder.layer.7.intermediate.dense.weight \t tensor([[-0.0057, -0.0269,  0.0121,  ...,  0.0410,  0.0068, -0.0894],\n",
      "        [-0.0095, -0.0227, -0.0070,  ..., -0.0074,  0.0112, -0.0589],\n",
      "        [-0.0199,  0.0036, -0.0132,  ...,  0.0087, -0.0013,  0.0383],\n",
      "        ...,\n",
      "        [ 0.0099,  0.0219, -0.0702,  ...,  0.0303,  0.0955, -0.0463],\n",
      "        [-0.0208,  0.0022,  0.0082,  ...,  0.0675,  0.0185, -0.0073],\n",
      "        [ 0.0047,  0.0330, -0.0214,  ..., -0.0057,  0.0153, -0.0911]])\n",
      "roberta.encoder.layer.7.intermediate.dense.bias \t tensor([-0.0271, -0.0151, -0.0249,  ..., -0.0444, -0.0023, -0.0200])\n",
      "roberta.encoder.layer.7.output.dense.weight \t tensor([[ 0.0300,  0.0190,  0.0342,  ..., -0.0438,  0.0322, -0.0148],\n",
      "        [-0.0100,  0.0375, -0.0439,  ..., -0.0382,  0.0019,  0.0129],\n",
      "        [-0.0007,  0.0487,  0.0030,  ...,  0.0067, -0.0082,  0.0395],\n",
      "        ...,\n",
      "        [ 0.0044, -0.0289,  0.0231,  ..., -0.0137, -0.0249, -0.0070],\n",
      "        [ 0.0107,  0.0014,  0.0230,  ..., -0.0038,  0.0114,  0.0359],\n",
      "        [-0.0238, -0.0497,  0.0168,  ...,  0.0386, -0.0159,  0.0203]])\n",
      "roberta.encoder.layer.7.output.dense.bias \t tensor([ 0.0008,  0.0383,  0.0166,  ...,  0.0176, -0.0085,  0.0145])\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight \t tensor([0.8138, 0.7384, 0.7823,  ..., 0.7025, 0.7541, 0.7483])\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias \t tensor([ 0.0192,  0.0142, -0.0649,  ...,  0.0438,  0.0230, -0.0094])\n",
      "roberta.encoder.layer.8.attention.self.query.weight \t tensor([[ 0.0109,  0.0054, -0.0096,  ...,  0.0248,  0.0085, -0.0367],\n",
      "        [ 0.0586, -0.0084,  0.0169,  ..., -0.0194,  0.0022, -0.0204],\n",
      "        [-0.0075, -0.0710,  0.0006,  ..., -0.0039, -0.0307, -0.0087],\n",
      "        ...,\n",
      "        [-0.0053,  0.0282,  0.0350,  ...,  0.0223, -0.0415,  0.0178],\n",
      "        [-0.0206,  0.0642,  0.0108,  ...,  0.0030, -0.0029,  0.0008],\n",
      "        [ 0.0282,  0.0438,  0.0668,  ...,  0.0575, -0.0548, -0.0029]])\n",
      "roberta.encoder.layer.8.attention.self.query.bias \t tensor([ 0.0186, -0.0059, -0.0376,  ..., -0.0059, -0.0029,  0.0144])\n",
      "roberta.encoder.layer.8.attention.self.key.weight \t tensor([[-0.0240,  0.0229, -0.0279,  ...,  0.0442, -0.0054, -0.0199],\n",
      "        [ 0.0044,  0.0051, -0.0044,  ...,  0.0099, -0.0354, -0.0165],\n",
      "        [ 0.0442, -0.0106,  0.0321,  ..., -0.0027,  0.0464,  0.0475],\n",
      "        ...,\n",
      "        [ 0.0301, -0.0043, -0.0202,  ..., -0.0498,  0.0330, -0.0095],\n",
      "        [ 0.0434,  0.0582,  0.0291,  ...,  0.0795,  0.0230, -0.0012],\n",
      "        [-0.0320, -0.0150, -0.0200,  ..., -0.0841, -0.0342, -0.0889]])\n",
      "roberta.encoder.layer.8.attention.self.key.bias \t tensor([-8.1931e-06, -1.1006e-04, -6.7106e-05,  ..., -3.0309e-04,\n",
      "         2.8760e-04, -2.5002e-05])\n",
      "roberta.encoder.layer.8.attention.self.value.weight \t tensor([[-0.0160, -0.0164,  0.0021,  ...,  0.0056,  0.0649,  0.0519],\n",
      "        [-0.0002,  0.0288, -0.0143,  ..., -0.0096,  0.0275, -0.0504],\n",
      "        [ 0.0095, -0.0282, -0.0266,  ..., -0.0424,  0.0130, -0.0117],\n",
      "        ...,\n",
      "        [ 0.0026,  0.0153, -0.0232,  ..., -0.0162, -0.0241, -0.0166],\n",
      "        [-0.0032,  0.0091, -0.0259,  ...,  0.0248, -0.0153, -0.0046],\n",
      "        [ 0.0046,  0.0167, -0.0433,  ..., -0.0023,  0.0094,  0.0049]])\n",
      "roberta.encoder.layer.8.attention.self.value.bias \t tensor([ 0.0006,  0.0048, -0.0005,  ...,  0.0116, -0.0025, -0.0068])\n",
      "roberta.encoder.layer.8.attention.output.dense.weight \t tensor([[-0.0168, -0.0065,  0.0237,  ...,  0.0080, -0.0202,  0.0379],\n",
      "        [ 0.0319,  0.0017, -0.0020,  ...,  0.0186, -0.0443,  0.0269],\n",
      "        [-0.0013,  0.0057,  0.0294,  ..., -0.0315, -0.0043, -0.0075],\n",
      "        ...,\n",
      "        [-0.0023, -0.0360,  0.0160,  ...,  0.0013,  0.0032,  0.0169],\n",
      "        [ 0.0049,  0.0246, -0.0008,  ..., -0.0122,  0.0034,  0.0261],\n",
      "        [-0.0403,  0.0343,  0.0312,  ..., -0.0154,  0.0479, -0.0228]])\n",
      "roberta.encoder.layer.8.attention.output.dense.bias \t tensor([-0.0049, -0.0022, -0.0110,  ...,  0.0011,  0.0067, -0.0175])\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight \t tensor([0.7888, 0.7672, 0.7911,  ..., 0.6959, 0.7371, 0.7420])\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias \t tensor([ 0.0328, -0.1613,  0.0904,  ...,  0.0398, -0.0529,  0.0354])\n",
      "roberta.encoder.layer.8.intermediate.dense.weight \t tensor([[ 0.0475,  0.0365, -0.0112,  ...,  0.0502, -0.0408, -0.0023],\n",
      "        [-0.0746,  0.0117, -0.0278,  ..., -0.0178, -0.0438, -0.0035],\n",
      "        [ 0.0325, -0.0412, -0.0176,  ..., -0.0297, -0.0168, -0.0474],\n",
      "        ...,\n",
      "        [ 0.0275,  0.0312,  0.0015,  ..., -0.0393, -0.0022,  0.0169],\n",
      "        [-0.0209, -0.0093, -0.0169,  ...,  0.0067, -0.0117,  0.0098],\n",
      "        [ 0.0222,  0.0295, -0.0213,  ...,  0.0370, -0.0467, -0.0075]])\n",
      "roberta.encoder.layer.8.intermediate.dense.bias \t tensor([-0.0385, -0.0308, -0.0308,  ..., -0.0272, -0.0184, -0.0242])\n",
      "roberta.encoder.layer.8.output.dense.weight \t tensor([[ 0.0128, -0.0277,  0.0227,  ..., -0.0156,  0.0045, -0.0167],\n",
      "        [-0.0095,  0.0126, -0.0176,  ..., -0.0097, -0.0062, -0.0677],\n",
      "        [-0.0068,  0.0069,  0.0198,  ..., -0.0096,  0.0119,  0.0199],\n",
      "        ...,\n",
      "        [-0.0607,  0.0139, -0.0416,  ..., -0.0045, -0.0113,  0.0107],\n",
      "        [-0.0350, -0.0181, -0.0156,  ...,  0.0381, -0.0015, -0.0173],\n",
      "        [-0.0156,  0.0304, -0.0233,  ..., -0.0163, -0.0051,  0.0323]])\n",
      "roberta.encoder.layer.8.output.dense.bias \t tensor([ 0.0259, -0.0160,  0.0234,  ...,  0.0647, -0.0111,  0.0105])\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight \t tensor([0.8284, 0.7348, 0.7952,  ..., 0.7083, 0.7643, 0.7450])\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias \t tensor([ 0.0017, -0.0037, -0.0367,  ...,  0.0173,  0.0138, -0.0360])\n",
      "roberta.encoder.layer.9.attention.self.query.weight \t tensor([[ 0.0539, -0.0858,  0.0024,  ..., -0.0392,  0.0031,  0.0173],\n",
      "        [-0.0712, -0.0063, -0.0101,  ..., -0.0178,  0.0178, -0.0109],\n",
      "        [ 0.0365,  0.0062, -0.0051,  ...,  0.0203,  0.0134,  0.0219],\n",
      "        ...,\n",
      "        [-0.0175, -0.0461,  0.0191,  ...,  0.0405,  0.0471, -0.0348],\n",
      "        [-0.0524,  0.0155,  0.0260,  ..., -0.0643,  0.0032,  0.0482],\n",
      "        [-0.0162,  0.0168, -0.0500,  ...,  0.0729, -0.0025,  0.0070]])\n",
      "roberta.encoder.layer.9.attention.self.query.bias \t tensor([-0.0241,  0.0783,  0.0904,  ...,  0.0062,  0.0094, -0.0569])\n",
      "roberta.encoder.layer.9.attention.self.key.weight \t tensor([[ 1.9786e-02, -1.4394e-02,  4.4224e-02,  ...,  3.4348e-02,\n",
      "          4.7961e-02, -4.8765e-02],\n",
      "        [ 2.3481e-02, -7.2078e-03, -4.3708e-02,  ..., -1.4312e-02,\n",
      "         -1.6187e-02, -4.7383e-02],\n",
      "        [ 6.4690e-03, -2.3168e-02, -2.2506e-02,  ...,  2.3798e-02,\n",
      "          2.0548e-02, -1.0265e-02],\n",
      "        ...,\n",
      "        [ 4.6208e-02, -1.5032e-02,  7.3846e-02,  ..., -1.1315e-02,\n",
      "         -2.7249e-02,  1.3014e-02],\n",
      "        [-2.4590e-02,  4.8965e-02,  3.7090e-03,  ..., -5.9297e-02,\n",
      "          9.2025e-02, -3.9055e-03],\n",
      "        [ 2.8107e-02, -5.1895e-02, -7.8375e-05,  ..., -1.3161e-02,\n",
      "         -2.6906e-02, -1.6642e-02]])\n",
      "roberta.encoder.layer.9.attention.self.key.bias \t tensor([ 1.3957e-06,  4.7649e-04,  9.1770e-05,  ...,  8.6820e-04,\n",
      "         5.1338e-04, -7.8693e-04])\n",
      "roberta.encoder.layer.9.attention.self.value.weight \t tensor([[ 0.0097,  0.0094,  0.0386,  ...,  0.0201,  0.0092, -0.0637],\n",
      "        [ 0.0172, -0.0090,  0.0064,  ...,  0.0024,  0.0163,  0.0092],\n",
      "        [ 0.0237, -0.0404,  0.0029,  ..., -0.0205, -0.0010, -0.0275],\n",
      "        ...,\n",
      "        [-0.0100, -0.0226, -0.0041,  ...,  0.0205, -0.0340, -0.0332],\n",
      "        [ 0.0066,  0.0150, -0.0143,  ..., -0.0084, -0.0238, -0.0343],\n",
      "        [-0.0117,  0.0194, -0.0151,  ..., -0.0519, -0.0218,  0.0155]])\n",
      "roberta.encoder.layer.9.attention.self.value.bias \t tensor([ 0.0034,  0.0018, -0.0114,  ...,  0.0219,  0.0042,  0.0176])\n",
      "roberta.encoder.layer.9.attention.output.dense.weight \t tensor([[ 0.0276, -0.0009, -0.0343,  ..., -0.0337, -0.0073, -0.0005],\n",
      "        [-0.0230, -0.0138, -0.0271,  ..., -0.0274, -0.0560,  0.0110],\n",
      "        [ 0.0091, -0.0034, -0.0039,  ..., -0.0151, -0.0128,  0.0043],\n",
      "        ...,\n",
      "        [ 0.0035,  0.0620,  0.0348,  ..., -0.0124,  0.0018, -0.0166],\n",
      "        [-0.0234,  0.0045, -0.0460,  ...,  0.0132,  0.0127, -0.0409],\n",
      "        [ 0.0050, -0.0248,  0.0025,  ..., -0.0609,  0.0366, -0.0104]])\n",
      "roberta.encoder.layer.9.attention.output.dense.bias \t tensor([ 0.0019,  0.0116, -0.0261,  ..., -0.0046,  0.0147,  0.0404])\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight \t tensor([0.8044, 0.7863, 0.7855,  ..., 0.6928, 0.7392, 0.7273])\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias \t tensor([ 0.0332, -0.1995,  0.0899,  ...,  0.0520, -0.0472, -0.0104])\n",
      "roberta.encoder.layer.9.intermediate.dense.weight \t tensor([[ 0.0080,  0.0163,  0.0331,  ..., -0.0061,  0.0196,  0.0343],\n",
      "        [ 0.0286, -0.0082,  0.0337,  ..., -0.0599,  0.0089, -0.0221],\n",
      "        [-0.0277, -0.0052,  0.0041,  ...,  0.0657, -0.0053, -0.0471],\n",
      "        ...,\n",
      "        [-0.0102,  0.0180, -0.0075,  ..., -0.0422, -0.0007,  0.0436],\n",
      "        [-0.0042, -0.0265,  0.0173,  ...,  0.0143,  0.0091, -0.0362],\n",
      "        [-0.0100,  0.0862, -0.0067,  ..., -0.0019,  0.0370,  0.0100]])\n",
      "roberta.encoder.layer.9.intermediate.dense.bias \t tensor([ 0.0080, -0.0190, -0.0269,  ..., -0.0094, -0.0106, -0.0377])\n",
      "roberta.encoder.layer.9.output.dense.weight \t tensor([[ 0.0059, -0.0100, -0.0196,  ..., -0.0534,  0.0413, -0.0094],\n",
      "        [-0.0872, -0.0175, -0.0394,  ...,  0.0040, -0.0010,  0.0116],\n",
      "        [ 0.0457,  0.0068,  0.0440,  ...,  0.0330, -0.0017,  0.0454],\n",
      "        ...,\n",
      "        [-0.0607,  0.0209, -0.0661,  ..., -0.0143, -0.0097,  0.0779],\n",
      "        [-0.0467,  0.0250, -0.0135,  ..., -0.0006,  0.0107, -0.0454],\n",
      "        [ 0.0282,  0.0226,  0.0426,  ...,  0.0076, -0.0157,  0.0202]])\n",
      "roberta.encoder.layer.9.output.dense.bias \t tensor([ 0.0548, -0.0555, -0.0123,  ...,  0.0235, -0.0190, -0.0023])\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight \t tensor([0.8270, 0.7415, 0.7903,  ..., 0.7316, 0.7654, 0.7555])\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias \t tensor([-0.0040,  0.0192, -0.0505,  ..., -0.0049,  0.0138, -0.0264])\n",
      "roberta.encoder.layer.10.attention.self.query.weight \t tensor([[-0.0367, -0.0506, -0.0045,  ...,  0.0420, -0.0144, -0.0026],\n",
      "        [-0.0032,  0.0015,  0.0189,  ...,  0.0181, -0.0152,  0.0413],\n",
      "        [-0.0361, -0.0378,  0.0345,  ...,  0.0625, -0.0485, -0.0065],\n",
      "        ...,\n",
      "        [ 0.0094,  0.0296,  0.0346,  ...,  0.0055,  0.0439, -0.0032],\n",
      "        [ 0.0048,  0.0323, -0.0097,  ...,  0.0248,  0.0181, -0.0188],\n",
      "        [-0.0156, -0.0198, -0.0233,  ..., -0.0061, -0.0105,  0.0564]])\n",
      "roberta.encoder.layer.10.attention.self.query.bias \t tensor([ 0.0948,  0.0210,  0.0189,  ..., -0.0086, -0.0024, -0.0061])\n",
      "roberta.encoder.layer.10.attention.self.key.weight \t tensor([[ 0.0289, -0.0045, -0.0390,  ...,  0.0071,  0.0008,  0.0216],\n",
      "        [ 0.0696,  0.0352, -0.0525,  ..., -0.0206, -0.0207,  0.0035],\n",
      "        [ 0.0004,  0.0275,  0.0058,  ..., -0.0054,  0.0036, -0.0275],\n",
      "        ...,\n",
      "        [-0.0040,  0.0108,  0.0208,  ...,  0.0428,  0.0400,  0.0061],\n",
      "        [ 0.0190,  0.0298, -0.0530,  ...,  0.0141,  0.0481, -0.0618],\n",
      "        [ 0.0231,  0.0106,  0.0641,  ...,  0.0185,  0.0290, -0.0208]])\n",
      "roberta.encoder.layer.10.attention.self.key.bias \t tensor([-5.2264e-04, -7.0357e-05, -2.4047e-04,  ...,  3.0926e-05,\n",
      "         1.4370e-04,  1.7994e-04])\n",
      "roberta.encoder.layer.10.attention.self.value.weight \t tensor([[-0.0305, -0.0244, -0.0193,  ..., -0.0576,  0.0154, -0.0020],\n",
      "        [-0.0428, -0.0208, -0.0060,  ..., -0.0036,  0.0020, -0.0329],\n",
      "        [-0.0280,  0.0123,  0.0267,  ..., -0.0172, -0.0552,  0.0424],\n",
      "        ...,\n",
      "        [ 0.0121, -0.0051,  0.0103,  ..., -0.0246, -0.0622, -0.0372],\n",
      "        [ 0.0156,  0.0350, -0.0350,  ...,  0.0105,  0.0007, -0.0570],\n",
      "        [ 0.0031,  0.0238, -0.0174,  ..., -0.0099,  0.0081, -0.0145]])\n",
      "roberta.encoder.layer.10.attention.self.value.bias \t tensor([ 0.0087, -0.0053, -0.0024,  ...,  0.0042,  0.0113, -0.0137])\n",
      "roberta.encoder.layer.10.attention.output.dense.weight \t tensor([[ 0.0258, -0.0167, -0.0080,  ...,  0.0049, -0.0181,  0.0070],\n",
      "        [-0.0008,  0.0288, -0.0137,  ...,  0.0014, -0.0198, -0.0212],\n",
      "        [ 0.0081,  0.0041, -0.0071,  ..., -0.0511,  0.0059, -0.0341],\n",
      "        ...,\n",
      "        [ 0.0182, -0.0430, -0.0308,  ...,  0.0583,  0.0023, -0.0245],\n",
      "        [ 0.0185, -0.0353,  0.0295,  ..., -0.0146, -0.0058, -0.0527],\n",
      "        [-0.0137, -0.0189,  0.0011,  ..., -0.0186,  0.0290,  0.0220]])\n",
      "roberta.encoder.layer.10.attention.output.dense.bias \t tensor([-0.0196,  0.0076, -0.0356,  ...,  0.0329,  0.0166,  0.0262])\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight \t tensor([0.7968, 0.7857, 0.7852,  ..., 0.6974, 0.7328, 0.7215])\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias \t tensor([ 0.0351, -0.2224,  0.0449,  ...,  0.0737, -0.0507, -0.0894])\n",
      "roberta.encoder.layer.10.intermediate.dense.weight \t tensor([[-0.0257,  0.0434, -0.0191,  ...,  0.0295, -0.0692,  0.0097],\n",
      "        [ 0.0027,  0.0399,  0.0250,  ..., -0.0224, -0.0088,  0.0471],\n",
      "        [ 0.0167,  0.0030,  0.0229,  ...,  0.0305, -0.0244,  0.0024],\n",
      "        ...,\n",
      "        [-0.0451,  0.0035,  0.0114,  ..., -0.0149, -0.0205,  0.0466],\n",
      "        [-0.0283, -0.0100,  0.0331,  ..., -0.0002, -0.0326,  0.0180],\n",
      "        [ 0.0355, -0.0320, -0.0025,  ...,  0.0040,  0.0291,  0.0574]])\n",
      "roberta.encoder.layer.10.intermediate.dense.bias \t tensor([-0.0191, -0.0233,  0.0129,  ..., -0.0179, -0.0219, -0.0375])\n",
      "roberta.encoder.layer.10.output.dense.weight \t tensor([[ 0.0164,  0.0245,  0.0052,  ..., -0.0419,  0.0186,  0.0010],\n",
      "        [ 0.0341, -0.0168,  0.0258,  ...,  0.0110,  0.0190,  0.0176],\n",
      "        [-0.0562,  0.0334,  0.0051,  ...,  0.0345,  0.0136, -0.0386],\n",
      "        ...,\n",
      "        [-0.0072,  0.0039,  0.0111,  ..., -0.0186, -0.0132, -0.0072],\n",
      "        [-0.0073, -0.0418,  0.0046,  ..., -0.0364, -0.0660, -0.0159],\n",
      "        [-0.0488, -0.0254, -0.0237,  ..., -0.0124, -0.0128, -0.0747]])\n",
      "roberta.encoder.layer.10.output.dense.bias \t tensor([ 0.0506, -0.0492, -0.0019,  ...,  0.0246,  0.0093, -0.0494])\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight \t tensor([0.8332, 0.7376, 0.8140,  ..., 0.7392, 0.7760, 0.7713])\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias \t tensor([-0.0091,  0.0497, -0.0374,  ..., -0.0122,  0.0263, -0.0038])\n",
      "roberta.encoder.layer.11.attention.self.query.weight \t tensor([[-8.5849e-03, -5.3916e-02,  4.0776e-02,  ...,  3.5116e-02,\n",
      "         -2.8495e-02, -2.1482e-02],\n",
      "        [-1.0317e-02, -2.1586e-02, -2.5151e-02,  ...,  4.4625e-02,\n",
      "         -2.1929e-02, -4.1826e-03],\n",
      "        [ 5.8183e-03,  8.2807e-03,  4.5548e-02,  ..., -4.8627e-02,\n",
      "         -2.1102e-02, -8.8404e-03],\n",
      "        ...,\n",
      "        [-1.9965e-02,  8.4706e-02,  4.8844e-02,  ..., -3.6872e-02,\n",
      "         -4.2923e-02, -2.4469e-02],\n",
      "        [-4.7860e-03, -7.8470e-03,  3.3515e-02,  ..., -2.6657e-02,\n",
      "          5.5710e-03, -5.1106e-05],\n",
      "        [ 5.3004e-03,  1.3571e-02,  5.3022e-02,  ...,  7.5928e-03,\n",
      "          2.0587e-02, -8.8177e-03]])\n",
      "roberta.encoder.layer.11.attention.self.query.bias \t tensor([ 0.0255,  0.0181, -0.0255,  ..., -0.0489, -0.0013, -0.0944])\n",
      "roberta.encoder.layer.11.attention.self.key.weight \t tensor([[ 0.0129,  0.0264,  0.0044,  ..., -0.0791,  0.0302, -0.0078],\n",
      "        [-0.0741, -0.0610, -0.0147,  ...,  0.0189,  0.0114, -0.0456],\n",
      "        [-0.0160, -0.0323,  0.0010,  ..., -0.0524, -0.0453, -0.0914],\n",
      "        ...,\n",
      "        [-0.0124, -0.0501,  0.0630,  ..., -0.0256, -0.0291,  0.0118],\n",
      "        [-0.0159,  0.0019, -0.0308,  ...,  0.0413, -0.0368, -0.0569],\n",
      "        [ 0.0233, -0.0572,  0.0245,  ..., -0.0084, -0.0215,  0.0639]])\n",
      "roberta.encoder.layer.11.attention.self.key.bias \t tensor([-1.0402e-05,  6.6559e-05, -4.3909e-05,  ...,  7.0048e-04,\n",
      "        -1.8819e-04,  1.7955e-04])\n",
      "roberta.encoder.layer.11.attention.self.value.weight \t tensor([[-0.0399, -0.0006,  0.0137,  ...,  0.0346,  0.0393,  0.0072],\n",
      "        [ 0.0073,  0.0361,  0.0388,  ...,  0.0202, -0.0256,  0.0275],\n",
      "        [-0.0062, -0.0057,  0.0020,  ...,  0.0120,  0.0103,  0.0169],\n",
      "        ...,\n",
      "        [ 0.0068,  0.0250, -0.0034,  ...,  0.0358,  0.0685, -0.0053],\n",
      "        [-0.0055,  0.0011,  0.0216,  ..., -0.0182,  0.0596,  0.0205],\n",
      "        [-0.0135, -0.0093,  0.0173,  ...,  0.0240,  0.0218,  0.0020]])\n",
      "roberta.encoder.layer.11.attention.self.value.bias \t tensor([ 0.0052,  0.0101, -0.0031,  ..., -0.0009, -0.0075, -0.0047])\n",
      "roberta.encoder.layer.11.attention.output.dense.weight \t tensor([[-0.0293,  0.0094,  0.0473,  ..., -0.0142, -0.0060, -0.0246],\n",
      "        [-0.0481, -0.0327, -0.0111,  ..., -0.0285, -0.0082,  0.0169],\n",
      "        [-0.0228,  0.0112, -0.0422,  ...,  0.0012, -0.0151, -0.0009],\n",
      "        ...,\n",
      "        [-0.0072, -0.0079, -0.0144,  ...,  0.0045,  0.0003, -0.0625],\n",
      "        [ 0.0627,  0.0277, -0.0352,  ..., -0.0123, -0.0403, -0.0363],\n",
      "        [ 0.0115, -0.0468, -0.0130,  ..., -0.0468, -0.0017, -0.0060]])\n",
      "roberta.encoder.layer.11.attention.output.dense.bias \t tensor([-0.0314, -0.0072, -0.0106,  ...,  0.0030,  0.0071,  0.0161])\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight \t tensor([0.7827, 0.7621, 0.7737,  ..., 0.6886, 0.6923, 0.7041])\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias \t tensor([ 0.0464, -0.1708,  0.0387,  ...,  0.1067,  0.0116, -0.0974])\n",
      "roberta.encoder.layer.11.intermediate.dense.weight \t tensor([[-0.0222, -0.0130,  0.0401,  ...,  0.0235, -0.0205,  0.0535],\n",
      "        [ 0.0293,  0.0130, -0.0357,  ..., -0.0165,  0.0481, -0.0119],\n",
      "        [ 0.0051,  0.0017, -0.0084,  ...,  0.0059, -0.0102,  0.0085],\n",
      "        ...,\n",
      "        [-0.0262,  0.0246, -0.0306,  ...,  0.0194,  0.0546, -0.0168],\n",
      "        [-0.0037, -0.0154, -0.0198,  ...,  0.0090,  0.0182, -0.0154],\n",
      "        [-0.0189, -0.0145, -0.0242,  ..., -0.0221, -0.0264, -0.0229]])\n",
      "roberta.encoder.layer.11.intermediate.dense.bias \t tensor([-0.0338, -0.0301, -0.0296,  ..., -0.0499, -0.0416, -0.0312])\n",
      "roberta.encoder.layer.11.output.dense.weight \t tensor([[ 0.0061,  0.0497, -0.0240,  ...,  0.0155,  0.0565, -0.0154],\n",
      "        [-0.0277,  0.0215, -0.0052,  ...,  0.0092, -0.0242, -0.0344],\n",
      "        [ 0.0272,  0.0059,  0.0119,  ..., -0.0543, -0.0140, -0.0088],\n",
      "        ...,\n",
      "        [-0.0679,  0.0166,  0.0240,  ...,  0.0271, -0.0466, -0.0045],\n",
      "        [ 0.0168,  0.0443, -0.0085,  ..., -0.0021,  0.0165, -0.0142],\n",
      "        [ 0.0394,  0.0098,  0.0339,  ...,  0.0471, -0.0660,  0.0130]])\n",
      "roberta.encoder.layer.11.output.dense.bias \t tensor([ 0.0443, -0.0502, -0.0047,  ...,  0.0095, -0.0204, -0.0786])\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight \t tensor([0.8388, 0.7482, 0.8234,  ..., 0.7531, 0.7740, 0.7666])\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias \t tensor([-0.0331,  0.0422, -0.0392,  ..., -0.0487, -0.0070,  0.0282])\n",
      "roberta.encoder.layer.12.attention.self.query.weight \t tensor([[ 0.0014, -0.0693, -0.0383,  ..., -0.0601, -0.0561,  0.0203],\n",
      "        [ 0.0304, -0.0144, -0.0036,  ...,  0.0031, -0.0280,  0.0193],\n",
      "        [-0.0059,  0.0242,  0.0154,  ..., -0.0109, -0.0184, -0.0209],\n",
      "        ...,\n",
      "        [-0.0055, -0.0270,  0.0382,  ..., -0.0042, -0.0176,  0.0082],\n",
      "        [ 0.0028, -0.0226, -0.0566,  ..., -0.0024,  0.0478,  0.0048],\n",
      "        [ 0.0053, -0.0128, -0.0262,  ..., -0.0055, -0.0192,  0.0643]])\n",
      "roberta.encoder.layer.12.attention.self.query.bias \t tensor([ 3.0653e-05,  2.3458e-03,  1.8124e-02,  ..., -2.7291e-02,\n",
      "         4.6453e-02, -3.2310e-02])\n",
      "roberta.encoder.layer.12.attention.self.key.weight \t tensor([[-0.0015,  0.0288,  0.0317,  ...,  0.0254, -0.0150,  0.0084],\n",
      "        [ 0.0886, -0.0195,  0.0299,  ..., -0.0270,  0.0357,  0.0067],\n",
      "        [ 0.0796,  0.0842,  0.0123,  ...,  0.0444,  0.0246, -0.0432],\n",
      "        ...,\n",
      "        [ 0.0106, -0.0213, -0.0721,  ..., -0.0238,  0.0427,  0.0022],\n",
      "        [ 0.0361,  0.0042,  0.0517,  ...,  0.0501,  0.0312, -0.0056],\n",
      "        [ 0.0477,  0.0264,  0.0398,  ..., -0.0043,  0.0508, -0.0343]])\n",
      "roberta.encoder.layer.12.attention.self.key.bias \t tensor([ 0.0002,  0.0004, -0.0008,  ...,  0.0002,  0.0004, -0.0003])\n",
      "roberta.encoder.layer.12.attention.self.value.weight \t tensor([[-0.0093,  0.0192, -0.0747,  ..., -0.0212,  0.0353,  0.0318],\n",
      "        [ 0.0188, -0.0336, -0.0284,  ...,  0.0446, -0.0220, -0.0071],\n",
      "        [ 0.0075, -0.0011,  0.0057,  ...,  0.0019,  0.0295, -0.0099],\n",
      "        ...,\n",
      "        [ 0.0046,  0.0426, -0.0144,  ..., -0.0255, -0.0169,  0.0389],\n",
      "        [ 0.0152,  0.0055, -0.0017,  ...,  0.0369,  0.0191,  0.0004],\n",
      "        [-0.0073,  0.0164, -0.0119,  ..., -0.0100,  0.0010, -0.0033]])\n",
      "roberta.encoder.layer.12.attention.self.value.bias \t tensor([-0.0148, -0.0149, -0.0156,  ..., -0.0004, -0.0053,  0.0073])\n",
      "roberta.encoder.layer.12.attention.output.dense.weight \t tensor([[-0.0037, -0.0138, -0.0058,  ...,  0.0091,  0.0047, -0.0071],\n",
      "        [ 0.0065,  0.0154,  0.0055,  ...,  0.0328,  0.0156, -0.0421],\n",
      "        [ 0.0595,  0.0294, -0.0127,  ..., -0.0023,  0.0175, -0.0262],\n",
      "        ...,\n",
      "        [-0.0176,  0.0080, -0.0170,  ..., -0.0292, -0.0402, -0.0106],\n",
      "        [ 0.0611, -0.0113,  0.0515,  ...,  0.0431, -0.0305, -0.0349],\n",
      "        [-0.0060, -0.0282,  0.0514,  ..., -0.0296, -0.0067,  0.0122]])\n",
      "roberta.encoder.layer.12.attention.output.dense.bias \t tensor([-0.0120, -0.0196, -0.0200,  ..., -0.0008, -0.0227, -0.0177])\n",
      "roberta.encoder.layer.12.attention.output.LayerNorm.weight \t tensor([0.7745, 0.7621, 0.7689,  ..., 0.7020, 0.7038, 0.7027])\n",
      "roberta.encoder.layer.12.attention.output.LayerNorm.bias \t tensor([ 0.0518, -0.1341,  0.0057,  ...,  0.0646, -0.0273, -0.0968])\n",
      "roberta.encoder.layer.12.intermediate.dense.weight \t tensor([[-0.0087, -0.0298, -0.0050,  ..., -0.0805, -0.0066,  0.0379],\n",
      "        [-0.0276, -0.0540,  0.0109,  ...,  0.0504,  0.0218,  0.0296],\n",
      "        [ 0.0546, -0.0074,  0.0315,  ..., -0.0394,  0.0373,  0.0480],\n",
      "        ...,\n",
      "        [-0.0195, -0.0150, -0.0106,  ..., -0.0164,  0.0099,  0.0305],\n",
      "        [-0.0181, -0.0055,  0.0087,  ..., -0.0095,  0.0250, -0.0079],\n",
      "        [-0.0336,  0.0498,  0.0297,  ..., -0.0505, -0.0484, -0.0327]])\n",
      "roberta.encoder.layer.12.intermediate.dense.bias \t tensor([-0.0179, -0.0068, -0.0241,  ..., -0.0329, -0.0304, -0.0190])\n",
      "roberta.encoder.layer.12.output.dense.weight \t tensor([[-0.0113, -0.0058,  0.0093,  ..., -0.0135,  0.0186, -0.0584],\n",
      "        [-0.0081, -0.0030, -0.0060,  ..., -0.0090,  0.0097,  0.0039],\n",
      "        [-0.0199, -0.0048, -0.0225,  ..., -0.0457,  0.0091,  0.0068],\n",
      "        ...,\n",
      "        [-0.0111,  0.0550,  0.0161,  ..., -0.0138,  0.0199,  0.0064],\n",
      "        [-0.0258, -0.0105,  0.0326,  ...,  0.0958,  0.0147,  0.0566],\n",
      "        [-0.0339,  0.0048,  0.0341,  ..., -0.0035,  0.0287,  0.0261]])\n",
      "roberta.encoder.layer.12.output.dense.bias \t tensor([ 0.0463, -0.0304, -0.0070,  ...,  0.0166,  0.0010, -0.0735])\n",
      "roberta.encoder.layer.12.output.LayerNorm.weight \t tensor([0.8444, 0.7548, 0.8193,  ..., 0.7635, 0.7809, 0.7740])\n",
      "roberta.encoder.layer.12.output.LayerNorm.bias \t tensor([-0.0431,  0.0407, -0.0400,  ..., -0.0272,  0.0263,  0.0414])\n",
      "roberta.encoder.layer.13.attention.self.query.weight \t tensor([[ 0.0624, -0.0569,  0.0299,  ...,  0.0165, -0.0238,  0.0048],\n",
      "        [ 0.0337, -0.0052, -0.0048,  ..., -0.0171,  0.0041,  0.0078],\n",
      "        [-0.0060, -0.0237, -0.0096,  ...,  0.0719, -0.0461,  0.0413],\n",
      "        ...,\n",
      "        [-0.0292,  0.0739, -0.0171,  ..., -0.0271,  0.0315,  0.0399],\n",
      "        [ 0.0009,  0.0803, -0.0005,  ..., -0.0184, -0.0137,  0.0128],\n",
      "        [ 0.0250,  0.0203, -0.0395,  ...,  0.0533, -0.0159, -0.0110]])\n",
      "roberta.encoder.layer.13.attention.self.query.bias \t tensor([-0.0146,  0.0130,  0.0330,  ...,  0.0214, -0.0431,  0.0075])\n",
      "roberta.encoder.layer.13.attention.self.key.weight \t tensor([[ 0.0284,  0.0225,  0.0462,  ...,  0.0105, -0.0415, -0.0088],\n",
      "        [-0.0230, -0.0518, -0.0022,  ..., -0.0225,  0.0002,  0.0085],\n",
      "        [ 0.0426, -0.0224,  0.0183,  ...,  0.0655,  0.0346,  0.0113],\n",
      "        ...,\n",
      "        [-0.0530,  0.0419, -0.0191,  ...,  0.0020, -0.0131,  0.0620],\n",
      "        [ 0.0127, -0.0502, -0.0485,  ...,  0.0190,  0.0119, -0.0238],\n",
      "        [-0.0183,  0.0204, -0.0011,  ..., -0.0426, -0.0343, -0.0341]])\n",
      "roberta.encoder.layer.13.attention.self.key.bias \t tensor([-6.2959e-06, -2.5621e-04, -7.2843e-06,  ..., -1.2541e-04,\n",
      "         5.1758e-05,  2.2323e-05])\n",
      "roberta.encoder.layer.13.attention.self.value.weight \t tensor([[ 0.0281, -0.0357,  0.0452,  ..., -0.0592,  0.0208, -0.0140],\n",
      "        [ 0.0245, -0.0461,  0.0164,  ..., -0.0147, -0.0409,  0.0002],\n",
      "        [-0.0066,  0.0145, -0.0158,  ..., -0.0141,  0.0134,  0.0453],\n",
      "        ...,\n",
      "        [ 0.0457,  0.0275, -0.0183,  ...,  0.0271,  0.0042, -0.0399],\n",
      "        [ 0.0574,  0.0113, -0.0013,  ...,  0.0448,  0.0219,  0.0005],\n",
      "        [ 0.0137, -0.0173,  0.0052,  ...,  0.0158,  0.0277,  0.0070]])\n",
      "roberta.encoder.layer.13.attention.self.value.bias \t tensor([0.0011, 0.0091, 0.0188,  ..., 0.0019, 0.0120, 0.0107])\n",
      "roberta.encoder.layer.13.attention.output.dense.weight \t tensor([[-0.0104,  0.0002,  0.0180,  ..., -0.0038, -0.0759, -0.0085],\n",
      "        [-0.0033, -0.0187,  0.0041,  ..., -0.0136,  0.0050,  0.0225],\n",
      "        [ 0.0094, -0.0015,  0.0299,  ...,  0.0245,  0.0250, -0.0015],\n",
      "        ...,\n",
      "        [ 0.0081, -0.0470, -0.0136,  ...,  0.0343, -0.0102, -0.0473],\n",
      "        [-0.0621, -0.0088, -0.0279,  ...,  0.0157,  0.0268,  0.0102],\n",
      "        [-0.0189, -0.0344, -0.0102,  ...,  0.0279, -0.0230,  0.0029]])\n",
      "roberta.encoder.layer.13.attention.output.dense.bias \t tensor([-0.0177, -0.0150,  0.0177,  ...,  0.0010, -0.0296, -0.0059])\n",
      "roberta.encoder.layer.13.attention.output.LayerNorm.weight \t tensor([0.7890, 0.7756, 0.7572,  ..., 0.7054, 0.7122, 0.7058])\n",
      "roberta.encoder.layer.13.attention.output.LayerNorm.bias \t tensor([ 0.0305, -0.1187, -0.0175,  ...,  0.0780,  0.0107, -0.0919])\n",
      "roberta.encoder.layer.13.intermediate.dense.weight \t tensor([[-0.0183,  0.0032, -0.0008,  ..., -0.0082, -0.0328,  0.0527],\n",
      "        [ 0.0017, -0.0035,  0.0171,  ..., -0.0040,  0.0045, -0.0031],\n",
      "        [ 0.0127, -0.0031, -0.0220,  ..., -0.0706,  0.0451, -0.0032],\n",
      "        ...,\n",
      "        [ 0.0198,  0.0454,  0.0044,  ...,  0.0090, -0.0266, -0.0077],\n",
      "        [-0.0164, -0.0268,  0.0027,  ..., -0.0292,  0.0249, -0.0055],\n",
      "        [-0.0141,  0.0137,  0.0616,  ...,  0.0102, -0.0554,  0.0069]])\n",
      "roberta.encoder.layer.13.intermediate.dense.bias \t tensor([-0.0362, -0.0183, -0.0396,  ..., -0.0331, -0.0137, -0.0663])\n",
      "roberta.encoder.layer.13.output.dense.weight \t tensor([[-0.0303, -0.0203,  0.0322,  ...,  0.0165, -0.0336,  0.0250],\n",
      "        [-0.0454, -0.0223,  0.0297,  ...,  0.0298,  0.0122,  0.0476],\n",
      "        [ 0.0107,  0.0061, -0.0101,  ..., -0.0182, -0.0242,  0.0700],\n",
      "        ...,\n",
      "        [-0.0636,  0.0144, -0.0041,  ...,  0.0124,  0.0047,  0.0512],\n",
      "        [-0.0326,  0.0146, -0.0493,  ...,  0.0358, -0.0333, -0.0698],\n",
      "        [ 0.0203, -0.0072, -0.0438,  ..., -0.0341, -0.0027, -0.0016]])\n",
      "roberta.encoder.layer.13.output.dense.bias \t tensor([ 0.0463, -0.0322, -0.0068,  ...,  0.0170, -0.0082, -0.0350])\n",
      "roberta.encoder.layer.13.output.LayerNorm.weight \t tensor([0.8692, 0.7876, 0.8509,  ..., 0.7900, 0.7981, 0.7992])\n",
      "roberta.encoder.layer.13.output.LayerNorm.bias \t tensor([-0.0089,  0.0497, -0.0198,  ..., -0.0473,  0.0004,  0.0417])\n",
      "roberta.encoder.layer.14.attention.self.query.weight \t tensor([[-0.0183,  0.0247,  0.0149,  ...,  0.0310, -0.0114, -0.0434],\n",
      "        [ 0.0429,  0.0390,  0.0488,  ...,  0.0036,  0.0410,  0.0099],\n",
      "        [ 0.0073, -0.0077, -0.0120,  ...,  0.0160, -0.0601, -0.0139],\n",
      "        ...,\n",
      "        [-0.0060,  0.0300,  0.0153,  ..., -0.0350, -0.0168, -0.0191],\n",
      "        [ 0.0311, -0.0334,  0.0112,  ...,  0.0223, -0.0593,  0.0229],\n",
      "        [-0.0296, -0.0254, -0.0673,  ..., -0.0123, -0.0175, -0.0589]])\n",
      "roberta.encoder.layer.14.attention.self.query.bias \t tensor([-0.0075,  0.0480,  0.0436,  ..., -0.0231,  0.0040,  0.0351])\n",
      "roberta.encoder.layer.14.attention.self.key.weight \t tensor([[-0.0253, -0.0214, -0.0191,  ...,  0.0059,  0.1077, -0.0408],\n",
      "        [ 0.0490,  0.0311,  0.0663,  ...,  0.0052, -0.0316,  0.0363],\n",
      "        [-0.0691,  0.0357,  0.0328,  ..., -0.0143,  0.0656,  0.0261],\n",
      "        ...,\n",
      "        [ 0.0097,  0.0023, -0.0674,  ..., -0.0161, -0.0122, -0.0089],\n",
      "        [ 0.0497,  0.0035,  0.0235,  ...,  0.0235, -0.0354, -0.1145],\n",
      "        [-0.0247,  0.0077, -0.0098,  ...,  0.0384,  0.0607,  0.0279]])\n",
      "roberta.encoder.layer.14.attention.self.key.bias \t tensor([-8.1125e-04,  5.9035e-04,  1.5707e-04,  ...,  8.2760e-05,\n",
      "        -3.7926e-06, -2.4492e-05])\n",
      "roberta.encoder.layer.14.attention.self.value.weight \t tensor([[-0.0128, -0.0013,  0.0244,  ...,  0.0266, -0.0162,  0.0261],\n",
      "        [-0.0115,  0.0081, -0.0332,  ...,  0.0041,  0.0435,  0.0401],\n",
      "        [ 0.0092,  0.0357, -0.0024,  ..., -0.0249, -0.0026,  0.0208],\n",
      "        ...,\n",
      "        [-0.0385, -0.0093, -0.0280,  ...,  0.0405, -0.0031, -0.0421],\n",
      "        [-0.0290,  0.0346, -0.0114,  ..., -0.0135, -0.0462,  0.0502],\n",
      "        [-0.0026, -0.0127, -0.0192,  ..., -0.0006,  0.0172, -0.0032]])\n",
      "roberta.encoder.layer.14.attention.self.value.bias \t tensor([-0.0183, -0.0076, -0.0201,  ...,  0.0022,  0.0003, -0.0029])\n",
      "roberta.encoder.layer.14.attention.output.dense.weight \t tensor([[-0.0002, -0.0193, -0.0179,  ...,  0.0481, -0.0101,  0.0111],\n",
      "        [-0.0233, -0.0062,  0.0049,  ..., -0.0249, -0.0086, -0.0310],\n",
      "        [ 0.0317, -0.0128, -0.0005,  ...,  0.0198,  0.0085,  0.0107],\n",
      "        ...,\n",
      "        [-0.0099, -0.0051, -0.0522,  ..., -0.0061,  0.0124, -0.0083],\n",
      "        [-0.0230,  0.0374,  0.0122,  ...,  0.0145,  0.0311,  0.0153],\n",
      "        [-0.0091,  0.0072, -0.0298,  ...,  0.0252, -0.0206, -0.0252]])\n",
      "roberta.encoder.layer.14.attention.output.dense.bias \t tensor([-0.0422,  0.0012,  0.0221,  ...,  0.0054, -0.0336, -0.0146])\n",
      "roberta.encoder.layer.14.attention.output.LayerNorm.weight \t tensor([0.7672, 0.7875, 0.7723,  ..., 0.7387, 0.7151, 0.7200])\n",
      "roberta.encoder.layer.14.attention.output.LayerNorm.bias \t tensor([ 0.0603, -0.1143, -0.0107,  ...,  0.0161,  0.0031, -0.0735])\n",
      "roberta.encoder.layer.14.intermediate.dense.weight \t tensor([[-0.0027,  0.0099,  0.0086,  ...,  0.0193,  0.0118,  0.0363],\n",
      "        [-0.0131, -0.0100, -0.0097,  ...,  0.0147, -0.0230, -0.0082],\n",
      "        [ 0.0119,  0.0375, -0.0192,  ..., -0.0730,  0.0526, -0.0533],\n",
      "        ...,\n",
      "        [-0.0242, -0.0346, -0.0513,  ..., -0.0448,  0.0331,  0.0231],\n",
      "        [ 0.0076,  0.0194, -0.0417,  ...,  0.0128, -0.0167, -0.0106],\n",
      "        [-0.0033,  0.0143,  0.0150,  ..., -0.0209, -0.0397,  0.0416]])\n",
      "roberta.encoder.layer.14.intermediate.dense.bias \t tensor([-0.0209, -0.0088, -0.0309,  ..., -0.0350, -0.0136, -0.0212])\n",
      "roberta.encoder.layer.14.output.dense.weight \t tensor([[ 0.0171,  0.0129,  0.0024,  ...,  0.0108,  0.0044, -0.0004],\n",
      "        [-0.0050, -0.0023, -0.0354,  ..., -0.0378,  0.0190,  0.0199],\n",
      "        [ 0.0084, -0.0050, -0.0415,  ..., -0.0070, -0.0068,  0.0127],\n",
      "        ...,\n",
      "        [-0.0486,  0.0137, -0.0211,  ..., -0.0233,  0.0158, -0.0122],\n",
      "        [ 0.0097, -0.0071,  0.0315,  ...,  0.0208,  0.0217, -0.0105],\n",
      "        [ 0.0114, -0.0063, -0.0125,  ..., -0.0099,  0.0273,  0.0071]])\n",
      "roberta.encoder.layer.14.output.dense.bias \t tensor([ 0.0732, -0.0295,  0.0120,  ..., -0.0109, -0.0171, -0.0334])\n",
      "roberta.encoder.layer.14.output.LayerNorm.weight \t tensor([0.8223, 0.7586, 0.8119,  ..., 0.7755, 0.7905, 0.7832])\n",
      "roberta.encoder.layer.14.output.LayerNorm.bias \t tensor([-0.0129,  0.0227, -0.0115,  ..., -0.0475,  0.0046,  0.0295])\n",
      "roberta.encoder.layer.15.attention.self.query.weight \t tensor([[-0.0005, -0.0294,  0.0349,  ..., -0.0029, -0.0230,  0.0074],\n",
      "        [-0.0144,  0.0320,  0.0275,  ..., -0.0058, -0.0425,  0.0483],\n",
      "        [ 0.0062,  0.0018, -0.0075,  ...,  0.0163,  0.0512, -0.0570],\n",
      "        ...,\n",
      "        [-0.0082, -0.0017,  0.0214,  ...,  0.0119, -0.0627, -0.0395],\n",
      "        [-0.0011,  0.0113,  0.0155,  ...,  0.0314, -0.1006,  0.0494],\n",
      "        [-0.0608,  0.0056, -0.0068,  ...,  0.0209, -0.0793, -0.0122]])\n",
      "roberta.encoder.layer.15.attention.self.query.bias \t tensor([-0.0125,  0.0029,  0.0191,  ..., -0.0190,  0.0507, -0.0126])\n",
      "roberta.encoder.layer.15.attention.self.key.weight \t tensor([[-8.4319e-03, -3.3283e-02, -3.9495e-04,  ..., -7.8546e-03,\n",
      "         -9.7961e-03, -1.1924e-02],\n",
      "        [-2.7066e-02, -1.2589e-02,  4.5739e-02,  ..., -1.3080e-02,\n",
      "         -2.9360e-02, -2.1409e-02],\n",
      "        [ 2.9710e-02,  7.3956e-05,  1.0218e-02,  ..., -1.7322e-02,\n",
      "         -5.0799e-02, -7.6555e-03],\n",
      "        ...,\n",
      "        [-8.6891e-02,  3.2201e-02,  1.0087e-02,  ...,  1.6718e-02,\n",
      "         -3.0063e-03,  4.7010e-03],\n",
      "        [ 4.0945e-02, -5.6122e-02,  2.2763e-03,  ..., -3.9776e-02,\n",
      "         -4.1568e-02, -2.6870e-02],\n",
      "        [ 8.7696e-04,  2.3928e-02, -2.5788e-02,  ...,  5.4748e-02,\n",
      "          5.1278e-03, -1.1718e-02]])\n",
      "roberta.encoder.layer.15.attention.self.key.bias \t tensor([-1.2787e-04,  5.0031e-05,  1.9263e-05,  ...,  1.9441e-04,\n",
      "         5.8255e-05, -6.9033e-05])\n",
      "roberta.encoder.layer.15.attention.self.value.weight \t tensor([[-0.0177, -0.0044,  0.0056,  ...,  0.0328, -0.0103, -0.0134],\n",
      "        [-0.0197,  0.0198, -0.0156,  ..., -0.0040, -0.0341,  0.0161],\n",
      "        [-0.0107,  0.0231, -0.0153,  ...,  0.0434, -0.0437, -0.0251],\n",
      "        ...,\n",
      "        [ 0.0346, -0.0337, -0.0447,  ...,  0.0212,  0.0305, -0.0028],\n",
      "        [ 0.0041, -0.0232,  0.0055,  ...,  0.0275,  0.0721, -0.0024],\n",
      "        [-0.0162, -0.0307,  0.0221,  ..., -0.0064,  0.0676,  0.0051]])\n",
      "roberta.encoder.layer.15.attention.self.value.bias \t tensor([ 0.0007,  0.0023,  0.0042,  ...,  0.0186,  0.0077, -0.0007])\n",
      "roberta.encoder.layer.15.attention.output.dense.weight \t tensor([[-0.0086,  0.0305,  0.0345,  ..., -0.0376,  0.0031,  0.0046],\n",
      "        [ 0.0162,  0.0027, -0.0320,  ...,  0.0241,  0.0230, -0.0125],\n",
      "        [ 0.0094,  0.0205,  0.0281,  ...,  0.0365, -0.0086,  0.0011],\n",
      "        ...,\n",
      "        [-0.0175,  0.0073,  0.0302,  ..., -0.0088,  0.0551, -0.0278],\n",
      "        [-0.0223,  0.0181, -0.0196,  ..., -0.0180, -0.0026,  0.0148],\n",
      "        [-0.0363, -0.0388, -0.0008,  ..., -0.0159, -0.0068,  0.0392]])\n",
      "roberta.encoder.layer.15.attention.output.dense.bias \t tensor([-0.0095,  0.0170,  0.0274,  ..., -0.0017, -0.0518,  0.0343])\n",
      "roberta.encoder.layer.15.attention.output.LayerNorm.weight \t tensor([0.7791, 0.7965, 0.7597,  ..., 0.7245, 0.7074, 0.7210])\n",
      "roberta.encoder.layer.15.attention.output.LayerNorm.bias \t tensor([ 0.0347, -0.1331, -0.0032,  ..., -0.0206,  0.0182, -0.0759])\n",
      "roberta.encoder.layer.15.intermediate.dense.weight \t tensor([[-0.0340, -0.0021,  0.0019,  ..., -0.0008, -0.0413, -0.0351],\n",
      "        [ 0.0138, -0.0027, -0.0139,  ...,  0.0024,  0.0235,  0.0061],\n",
      "        [-0.0118, -0.0096,  0.0566,  ..., -0.0090, -0.0535, -0.0083],\n",
      "        ...,\n",
      "        [ 0.0076,  0.0217,  0.0316,  ..., -0.0344, -0.0431, -0.0105],\n",
      "        [-0.0008,  0.0139, -0.0613,  ..., -0.0016,  0.0066,  0.0388],\n",
      "        [-0.0277, -0.0204,  0.0021,  ...,  0.0408,  0.0171,  0.0912]])\n",
      "roberta.encoder.layer.15.intermediate.dense.bias \t tensor([-0.0355, -0.0014, -0.0011,  ..., -0.0372, -0.0389, -0.0517])\n",
      "roberta.encoder.layer.15.output.dense.weight \t tensor([[ 0.0035, -0.0032,  0.0183,  ...,  0.0185,  0.0189,  0.0152],\n",
      "        [ 0.0634,  0.0065,  0.0173,  ..., -0.0437,  0.0230, -0.0412],\n",
      "        [-0.0168,  0.0141, -0.0299,  ...,  0.0758,  0.0079, -0.0047],\n",
      "        ...,\n",
      "        [ 0.0385,  0.0015, -0.0016,  ..., -0.0865,  0.0012, -0.0617],\n",
      "        [ 0.0250,  0.0046,  0.0514,  ..., -0.0510,  0.0273,  0.0044],\n",
      "        [-0.0785, -0.0273, -0.0214,  ...,  0.0253, -0.0010,  0.0279]])\n",
      "roberta.encoder.layer.15.output.dense.bias \t tensor([ 0.0499, -0.0447,  0.0230,  ..., -0.0203, -0.0151, -0.0195])\n",
      "roberta.encoder.layer.15.output.LayerNorm.weight \t tensor([0.8056, 0.7386, 0.8051,  ..., 0.7790, 0.7754, 0.7655])\n",
      "roberta.encoder.layer.15.output.LayerNorm.bias \t tensor([-0.0317,  0.0460, -0.0034,  ..., -0.0507,  0.0080,  0.0320])\n",
      "roberta.encoder.layer.16.attention.self.query.weight \t tensor([[ 0.0866, -0.0472,  0.0400,  ..., -0.0386,  0.0775, -0.0369],\n",
      "        [-0.0774, -0.0262, -0.0162,  ...,  0.0155,  0.0831, -0.0333],\n",
      "        [ 0.0205,  0.0652,  0.0841,  ..., -0.0117, -0.0109, -0.0364],\n",
      "        ...,\n",
      "        [-0.0044,  0.0375, -0.0026,  ...,  0.0090, -0.0444,  0.0136],\n",
      "        [ 0.0382, -0.0202, -0.0013,  ..., -0.0100, -0.0491, -0.0596],\n",
      "        [-0.0679,  0.0078, -0.0780,  ..., -0.0489, -0.0035,  0.0199]])\n",
      "roberta.encoder.layer.16.attention.self.query.bias \t tensor([ 0.0134,  0.0185, -0.0439,  ...,  0.0631, -0.0118, -0.0159])\n",
      "roberta.encoder.layer.16.attention.self.key.weight \t tensor([[ 0.0136, -0.0009, -0.0341,  ..., -0.0217,  0.0285,  0.0447],\n",
      "        [-0.0229, -0.0173,  0.0410,  ...,  0.0388,  0.0404,  0.0215],\n",
      "        [ 0.0177,  0.0113,  0.0765,  ..., -0.0920,  0.0175, -0.0247],\n",
      "        ...,\n",
      "        [-0.0165, -0.0153,  0.0071,  ..., -0.0695, -0.0411,  0.0192],\n",
      "        [ 0.0748,  0.0467,  0.0607,  ...,  0.0164,  0.0575, -0.0346],\n",
      "        [ 0.0389, -0.0259,  0.0189,  ...,  0.0612,  0.0436,  0.0437]])\n",
      "roberta.encoder.layer.16.attention.self.key.bias \t tensor([ 2.5240e-04,  4.4097e-04, -3.4494e-05,  ..., -5.4961e-04,\n",
      "         1.0490e-06,  6.9140e-04])\n",
      "roberta.encoder.layer.16.attention.self.value.weight \t tensor([[ 0.0185,  0.0166, -0.0083,  ..., -0.0053,  0.0239, -0.0039],\n",
      "        [-0.0302,  0.0080, -0.0380,  ..., -0.0039,  0.0141, -0.0095],\n",
      "        [ 0.0241, -0.0210,  0.0317,  ..., -0.0183, -0.0143, -0.0245],\n",
      "        ...,\n",
      "        [ 0.0257, -0.0039, -0.0129,  ..., -0.0005, -0.0091, -0.0539],\n",
      "        [ 0.0104, -0.0310,  0.0302,  ..., -0.0424,  0.0068,  0.0003],\n",
      "        [ 0.0286,  0.0134,  0.0087,  ...,  0.0089, -0.0404,  0.0402]])\n",
      "roberta.encoder.layer.16.attention.self.value.bias \t tensor([ 0.0147, -0.0071,  0.0143,  ...,  0.0168,  0.0056, -0.0136])\n",
      "roberta.encoder.layer.16.attention.output.dense.weight \t tensor([[-0.0045,  0.0177, -0.0125,  ...,  0.0209, -0.0100,  0.0018],\n",
      "        [-0.0455, -0.0260, -0.0092,  ...,  0.0326, -0.0200,  0.0196],\n",
      "        [ 0.0099,  0.0166,  0.0049,  ..., -0.0049, -0.0360,  0.0098],\n",
      "        ...,\n",
      "        [-0.0368,  0.0385, -0.0095,  ...,  0.0168,  0.0026,  0.0047],\n",
      "        [-0.0077,  0.0024,  0.0192,  ..., -0.0263,  0.0130,  0.0095],\n",
      "        [-0.0414,  0.0506,  0.0117,  ...,  0.0358,  0.0082, -0.0434]])\n",
      "roberta.encoder.layer.16.attention.output.dense.bias \t tensor([-0.0031, -0.0309, -0.0053,  ..., -0.0139, -0.0141, -0.0229])\n",
      "roberta.encoder.layer.16.attention.output.LayerNorm.weight \t tensor([0.7925, 0.8080, 0.7705,  ..., 0.7459, 0.7261, 0.7423])\n",
      "roberta.encoder.layer.16.attention.output.LayerNorm.bias \t tensor([ 0.0185, -0.1082,  0.0055,  ..., -0.0514,  0.0363, -0.0729])\n",
      "roberta.encoder.layer.16.intermediate.dense.weight \t tensor([[ 0.0266,  0.0512,  0.0109,  ...,  0.0281,  0.0338,  0.0255],\n",
      "        [ 0.0062,  0.0551, -0.0115,  ...,  0.0359,  0.0474,  0.0566],\n",
      "        [-0.0081,  0.0375, -0.0215,  ...,  0.0453, -0.0372,  0.0314],\n",
      "        ...,\n",
      "        [ 0.0012, -0.0113,  0.0011,  ..., -0.0206, -0.0425,  0.0143],\n",
      "        [ 0.0364,  0.0139, -0.0152,  ..., -0.0337, -0.0254, -0.0022],\n",
      "        [-0.0242,  0.0398,  0.0418,  ..., -0.0135, -0.0172, -0.0481]])\n",
      "roberta.encoder.layer.16.intermediate.dense.bias \t tensor([-0.0707, -0.0177, -0.0498,  ..., -0.0031, -0.0336, -0.0459])\n",
      "roberta.encoder.layer.16.output.dense.weight \t tensor([[-0.0055, -0.0154, -0.0377,  ...,  0.0035, -0.0241, -0.0273],\n",
      "        [ 0.0082,  0.0342,  0.0549,  ...,  0.0079,  0.0166,  0.0233],\n",
      "        [ 0.0330,  0.0017,  0.0273,  ..., -0.0115, -0.0481, -0.0370],\n",
      "        ...,\n",
      "        [ 0.0021, -0.0211,  0.0502,  ...,  0.0033,  0.0253, -0.0198],\n",
      "        [ 0.0301,  0.0924, -0.0133,  ...,  0.0282, -0.0003, -0.0511],\n",
      "        [ 0.0028, -0.0379, -0.0200,  ..., -0.0078, -0.0129,  0.0286]])\n",
      "roberta.encoder.layer.16.output.dense.bias \t tensor([ 0.0379, -0.0421,  0.0287,  ..., -0.0415,  0.0113, -0.0354])\n",
      "roberta.encoder.layer.16.output.LayerNorm.weight \t tensor([0.8172, 0.7526, 0.8142,  ..., 0.7842, 0.7639, 0.7807])\n",
      "roberta.encoder.layer.16.output.LayerNorm.bias \t tensor([-0.0061,  0.0433, -0.0142,  ..., -0.0310, -0.0005,  0.0096])\n",
      "roberta.encoder.layer.17.attention.self.query.weight \t tensor([[ 0.0389,  0.0296,  0.0084,  ..., -0.0326,  0.0294,  0.0398],\n",
      "        [ 0.0359,  0.0507, -0.0290,  ..., -0.0044, -0.0003, -0.0050],\n",
      "        [ 0.0146, -0.0018,  0.0043,  ...,  0.0296, -0.0213,  0.0126],\n",
      "        ...,\n",
      "        [ 0.0163, -0.0310,  0.0422,  ...,  0.0312, -0.0038, -0.0136],\n",
      "        [ 0.0296, -0.0313, -0.0057,  ..., -0.0648,  0.0362, -0.0455],\n",
      "        [-0.0178,  0.0374,  0.0130,  ..., -0.0378,  0.0766,  0.0008]])\n",
      "roberta.encoder.layer.17.attention.self.query.bias \t tensor([ 0.0551, -0.0337, -0.1062,  ..., -0.0022,  0.1166,  0.0322])\n",
      "roberta.encoder.layer.17.attention.self.key.weight \t tensor([[-0.0013,  0.0245, -0.0201,  ...,  0.0206,  0.0176,  0.0271],\n",
      "        [ 0.0480, -0.0175, -0.0428,  ...,  0.0360,  0.0054, -0.0067],\n",
      "        [ 0.0550,  0.0230, -0.0293,  ...,  0.0340, -0.0759, -0.0194],\n",
      "        ...,\n",
      "        [-0.0023,  0.0069,  0.0112,  ...,  0.0341,  0.0357,  0.0321],\n",
      "        [-0.0123,  0.0650, -0.0065,  ..., -0.0077,  0.0193,  0.0048],\n",
      "        [ 0.0504,  0.0334, -0.0225,  ...,  0.0105,  0.0417, -0.0216]])\n",
      "roberta.encoder.layer.17.attention.self.key.bias \t tensor([ 0.0001,  0.0003, -0.0012,  ...,  0.0003, -0.0009,  0.0005])\n",
      "roberta.encoder.layer.17.attention.self.value.weight \t tensor([[ 9.0547e-03,  4.4278e-02,  1.2152e-02,  ...,  7.2958e-03,\n",
      "          8.3208e-02,  3.0130e-02],\n",
      "        [ 2.8072e-02,  1.4683e-02,  6.9030e-03,  ..., -2.3532e-02,\n",
      "          7.2514e-04, -2.5007e-02],\n",
      "        [ 5.4459e-03,  5.1092e-02, -3.4077e-02,  ...,  2.8944e-02,\n",
      "         -8.2786e-03,  3.2688e-02],\n",
      "        ...,\n",
      "        [ 1.8898e-02, -3.5952e-03,  4.9027e-03,  ...,  4.2111e-02,\n",
      "         -9.5063e-02, -3.0588e-02],\n",
      "        [-1.8284e-02, -1.3701e-02,  4.2196e-02,  ..., -1.6429e-02,\n",
      "          4.5620e-03,  2.4150e-02],\n",
      "        [-8.5552e-05,  2.6291e-02,  5.2549e-02,  ..., -8.8554e-03,\n",
      "          8.5839e-03, -4.4196e-02]])\n",
      "roberta.encoder.layer.17.attention.self.value.bias \t tensor([ 0.0029, -0.0201, -0.0137,  ..., -0.0072, -0.0003,  0.0130])\n",
      "roberta.encoder.layer.17.attention.output.dense.weight \t tensor([[ 0.0208,  0.0544, -0.0083,  ..., -0.0280, -0.0038,  0.0116],\n",
      "        [-0.0345,  0.0112,  0.0702,  ..., -0.0169,  0.0208,  0.0090],\n",
      "        [ 0.0316,  0.0401, -0.0214,  ...,  0.0311, -0.0134, -0.0240],\n",
      "        ...,\n",
      "        [ 0.0008,  0.0254, -0.0328,  ...,  0.0155,  0.0092,  0.0374],\n",
      "        [ 0.0823, -0.0032, -0.0108,  ...,  0.0612,  0.0199,  0.0175],\n",
      "        [ 0.0237,  0.0386,  0.0367,  ...,  0.0049,  0.0092, -0.0248]])\n",
      "roberta.encoder.layer.17.attention.output.dense.bias \t tensor([ 0.0179, -0.0049, -0.0090,  ...,  0.0217, -0.0792,  0.0608])\n",
      "roberta.encoder.layer.17.attention.output.LayerNorm.weight \t tensor([0.8012, 0.8052, 0.7823,  ..., 0.7660, 0.7463, 0.7415])\n",
      "roberta.encoder.layer.17.attention.output.LayerNorm.bias \t tensor([ 0.0048, -0.1282, -0.0002,  ..., -0.0865,  0.0427, -0.0484])\n",
      "roberta.encoder.layer.17.intermediate.dense.weight \t tensor([[-0.0191, -0.0075, -0.0075,  ...,  0.0635,  0.0330, -0.0403],\n",
      "        [-0.0367,  0.0047, -0.0066,  ...,  0.0459,  0.0329, -0.0183],\n",
      "        [-0.0167, -0.0015, -0.0055,  ..., -0.0085, -0.0162,  0.0149],\n",
      "        ...,\n",
      "        [-0.0130, -0.0033, -0.0326,  ...,  0.0037,  0.0194, -0.0056],\n",
      "        [-0.0182,  0.0227,  0.0044,  ...,  0.0043, -0.0321, -0.0050],\n",
      "        [-0.0001, -0.0580, -0.0165,  ..., -0.0182,  0.0335,  0.0242]])\n",
      "roberta.encoder.layer.17.intermediate.dense.bias \t tensor([-0.0454, -0.0277, -0.0144,  ...,  0.0013, -0.0598, -0.0387])\n",
      "roberta.encoder.layer.17.output.dense.weight \t tensor([[-0.0050,  0.0118, -0.0122,  ..., -0.0196, -0.0110,  0.0067],\n",
      "        [ 0.0239,  0.0134, -0.0358,  ...,  0.0067,  0.0002, -0.0090],\n",
      "        [-0.0185, -0.0048,  0.0107,  ...,  0.0356,  0.0048, -0.0196],\n",
      "        ...,\n",
      "        [ 0.0121,  0.0247,  0.0044,  ..., -0.0150,  0.0047,  0.0185],\n",
      "        [ 0.0179, -0.0148,  0.0136,  ..., -0.0159,  0.0280, -0.0019],\n",
      "        [-0.0514, -0.0054,  0.0015,  ...,  0.0070, -0.0314, -0.0340]])\n",
      "roberta.encoder.layer.17.output.dense.bias \t tensor([ 0.0296, -0.0431,  0.0175,  ..., -0.0208,  0.0096, -0.0269])\n",
      "roberta.encoder.layer.17.output.LayerNorm.weight \t tensor([0.8196, 0.7582, 0.8095,  ..., 0.7897, 0.7569, 0.7826])\n",
      "roberta.encoder.layer.17.output.LayerNorm.bias \t tensor([ 0.0139,  0.0437, -0.0199,  ...,  0.0051, -0.0339,  0.0125])\n",
      "roberta.encoder.layer.18.attention.self.query.weight \t tensor([[-0.0638,  0.0195,  0.0022,  ...,  0.0234, -0.0803, -0.0297],\n",
      "        [-0.0419, -0.0043,  0.0334,  ...,  0.0196,  0.0149,  0.0248],\n",
      "        [ 0.0056, -0.0177,  0.0211,  ...,  0.0263,  0.0311,  0.0599],\n",
      "        ...,\n",
      "        [ 0.0244,  0.0102,  0.0034,  ...,  0.0239,  0.0478,  0.0177],\n",
      "        [-0.0059, -0.0680, -0.0163,  ..., -0.0480, -0.0488,  0.0050],\n",
      "        [-0.0166, -0.0022,  0.0012,  ..., -0.0222,  0.0103,  0.0103]])\n",
      "roberta.encoder.layer.18.attention.self.query.bias \t tensor([ 0.0141,  0.0175,  0.0070,  ...,  0.0397,  0.0367, -0.0292])\n",
      "roberta.encoder.layer.18.attention.self.key.weight \t tensor([[-0.0601, -0.0108,  0.0154,  ..., -0.0085, -0.0248, -0.0582],\n",
      "        [ 0.0039, -0.0185,  0.0696,  ..., -0.0422,  0.0225,  0.0300],\n",
      "        [ 0.0387,  0.0125,  0.0144,  ...,  0.0121, -0.0070, -0.0232],\n",
      "        ...,\n",
      "        [ 0.0662, -0.0469,  0.0425,  ..., -0.0484,  0.0907, -0.0654],\n",
      "        [-0.0107, -0.0016, -0.0261,  ..., -0.0096,  0.0059, -0.0152],\n",
      "        [ 0.0454, -0.0755, -0.0227,  ..., -0.0063,  0.0050, -0.0090]])\n",
      "roberta.encoder.layer.18.attention.self.key.bias \t tensor([ 0.0014,  0.0005,  0.0007,  ...,  0.0007,  0.0005, -0.0005])\n",
      "roberta.encoder.layer.18.attention.self.value.weight \t tensor([[ 0.0306, -0.0157, -0.0095,  ..., -0.0299, -0.0353,  0.0094],\n",
      "        [ 0.0317,  0.0202,  0.0065,  ...,  0.0294,  0.0539,  0.0077],\n",
      "        [-0.0012,  0.0053,  0.0006,  ..., -0.0130, -0.0088,  0.0354],\n",
      "        ...,\n",
      "        [ 0.0132,  0.0002, -0.0513,  ..., -0.0249,  0.0004,  0.0167],\n",
      "        [ 0.0242,  0.0033, -0.0369,  ...,  0.0169,  0.0318,  0.0072],\n",
      "        [-0.0158, -0.0351, -0.0106,  ...,  0.0054,  0.0215, -0.0002]])\n",
      "roberta.encoder.layer.18.attention.self.value.bias \t tensor([ 0.0065, -0.0025,  0.0041,  ..., -0.0041,  0.0140,  0.0074])\n",
      "roberta.encoder.layer.18.attention.output.dense.weight \t tensor([[ 0.0230, -0.0122,  0.0229,  ..., -0.0044,  0.0134,  0.0112],\n",
      "        [ 0.0274,  0.0090,  0.0088,  ...,  0.0441, -0.0273,  0.0091],\n",
      "        [ 0.0215, -0.0046, -0.0274,  ..., -0.0025, -0.0458,  0.0151],\n",
      "        ...,\n",
      "        [ 0.0269, -0.0038, -0.0276,  ..., -0.0407,  0.0017,  0.0246],\n",
      "        [-0.0527,  0.0164,  0.0406,  ..., -0.0394,  0.0335,  0.0216],\n",
      "        [ 0.0403, -0.0493, -0.0260,  ...,  0.0121,  0.0216,  0.0025]])\n",
      "roberta.encoder.layer.18.attention.output.dense.bias \t tensor([ 0.0073, -0.0067,  0.0123,  ..., -0.0285, -0.0932,  0.0401])\n",
      "roberta.encoder.layer.18.attention.output.LayerNorm.weight \t tensor([0.7899, 0.7978, 0.7750,  ..., 0.7765, 0.7237, 0.7464])\n",
      "roberta.encoder.layer.18.attention.output.LayerNorm.bias \t tensor([ 0.0118, -0.1120, -0.0022,  ..., -0.0711,  0.0417, -0.0306])\n",
      "roberta.encoder.layer.18.intermediate.dense.weight \t tensor([[-0.0172,  0.0112,  0.0350,  ..., -0.0174, -0.0042, -0.0139],\n",
      "        [-0.0348, -0.0141,  0.0268,  ...,  0.0164, -0.0136, -0.0300],\n",
      "        [-0.0039,  0.0232,  0.0505,  ..., -0.0263, -0.0267,  0.0486],\n",
      "        ...,\n",
      "        [-0.0219,  0.0286, -0.0198,  ...,  0.0096, -0.0294,  0.0381],\n",
      "        [ 0.0497, -0.0262,  0.0342,  ..., -0.0180,  0.0206, -0.0081],\n",
      "        [ 0.0138, -0.0061,  0.0365,  ..., -0.0072,  0.0080, -0.0508]])\n",
      "roberta.encoder.layer.18.intermediate.dense.bias \t tensor([-0.0081, -0.0094, -0.0066,  ..., -0.0294, -0.0056, -0.0158])\n",
      "roberta.encoder.layer.18.output.dense.weight \t tensor([[ 0.0009,  0.0331, -0.0093,  ..., -0.0030, -0.0272,  0.0038],\n",
      "        [-0.0215, -0.0037, -0.0315,  ..., -0.0387, -0.0167,  0.0118],\n",
      "        [-0.0120,  0.0303, -0.0124,  ...,  0.0010,  0.0197, -0.0346],\n",
      "        ...,\n",
      "        [-0.0031, -0.0109, -0.0032,  ..., -0.0188,  0.0451, -0.0253],\n",
      "        [ 0.0071, -0.0017,  0.0370,  ..., -0.0123, -0.0297,  0.0451],\n",
      "        [-0.0180, -0.0105,  0.0257,  ..., -0.0363,  0.0438, -0.0043]])\n",
      "roberta.encoder.layer.18.output.dense.bias \t tensor([ 0.0185, -0.0632,  0.0088,  ..., -0.0272,  0.0301, -0.0198])\n",
      "roberta.encoder.layer.18.output.LayerNorm.weight \t tensor([0.8169, 0.7905, 0.8150,  ..., 0.8043, 0.7886, 0.7852])\n",
      "roberta.encoder.layer.18.output.LayerNorm.bias \t tensor([ 0.0011,  0.0095, -0.0093,  ..., -0.0121, -0.0162,  0.0058])\n",
      "roberta.encoder.layer.19.attention.self.query.weight \t tensor([[ 0.0203, -0.0130, -0.0499,  ..., -0.0491, -0.0475,  0.0320],\n",
      "        [-0.0219, -0.0221,  0.0088,  ..., -0.0118, -0.0794,  0.0356],\n",
      "        [-0.0039,  0.0563,  0.0202,  ...,  0.0092,  0.1148, -0.0182],\n",
      "        ...,\n",
      "        [ 0.0581,  0.0341,  0.0429,  ...,  0.0064, -0.0303,  0.0184],\n",
      "        [ 0.0085, -0.0182,  0.0319,  ..., -0.0195,  0.0451,  0.0522],\n",
      "        [ 0.0151, -0.0872,  0.0433,  ...,  0.0037, -0.0263,  0.0440]])\n",
      "roberta.encoder.layer.19.attention.self.query.bias \t tensor([ 0.0121, -0.1069,  0.0284,  ...,  0.0191,  0.0226,  0.0703])\n",
      "roberta.encoder.layer.19.attention.self.key.weight \t tensor([[ 0.0290, -0.0281, -0.0618,  ..., -0.0016, -0.0301, -0.1076],\n",
      "        [-0.0467, -0.0844,  0.0068,  ..., -0.0429, -0.0478,  0.0378],\n",
      "        [-0.0147,  0.0670, -0.0742,  ..., -0.0245, -0.0651,  0.0094],\n",
      "        ...,\n",
      "        [ 0.0430,  0.0481, -0.0117,  ..., -0.0265,  0.0195, -0.0088],\n",
      "        [ 0.0425, -0.0591,  0.0252,  ...,  0.0250,  0.0010, -0.0045],\n",
      "        [ 0.0248,  0.0280,  0.0153,  ..., -0.0578,  0.0225, -0.0051]])\n",
      "roberta.encoder.layer.19.attention.self.key.bias \t tensor([ 6.7169e-04, -2.2071e-03,  2.7962e-04,  ...,  3.4920e-04,\n",
      "        -9.1192e-05,  2.4229e-04])\n",
      "roberta.encoder.layer.19.attention.self.value.weight \t tensor([[ 0.0197,  0.0070,  0.0217,  ..., -0.0354, -0.0187, -0.0116],\n",
      "        [-0.0033,  0.0108, -0.0065,  ..., -0.0272, -0.0165,  0.0022],\n",
      "        [-0.0398, -0.0483, -0.0210,  ...,  0.0322, -0.0225,  0.0174],\n",
      "        ...,\n",
      "        [ 0.0383,  0.0005, -0.0259,  ..., -0.0092,  0.0334, -0.0242],\n",
      "        [-0.0126, -0.0048,  0.0158,  ...,  0.0140, -0.0141,  0.0148],\n",
      "        [ 0.0065, -0.0115,  0.0265,  ...,  0.0090, -0.0299, -0.0037]])\n",
      "roberta.encoder.layer.19.attention.self.value.bias \t tensor([-0.0045, -0.0072,  0.0028,  ...,  0.0179, -0.0064,  0.0079])\n",
      "roberta.encoder.layer.19.attention.output.dense.weight \t tensor([[-0.0025, -0.0045, -0.0065,  ...,  0.0100,  0.0562, -0.0020],\n",
      "        [-0.0168,  0.0253, -0.0445,  ..., -0.0038,  0.0406,  0.0101],\n",
      "        [ 0.0091,  0.0039,  0.0195,  ..., -0.0080, -0.0290,  0.0025],\n",
      "        ...,\n",
      "        [-0.0088,  0.0082, -0.0043,  ...,  0.0285, -0.0026,  0.0021],\n",
      "        [-0.0290, -0.0333, -0.0214,  ..., -0.0104,  0.0327,  0.0279],\n",
      "        [ 0.0119,  0.0007,  0.0261,  ...,  0.0071,  0.0308,  0.0463]])\n",
      "roberta.encoder.layer.19.attention.output.dense.bias \t tensor([-0.0131, -0.0189, -0.0190,  ..., -0.0008, -0.0029, -0.0060])\n",
      "roberta.encoder.layer.19.attention.output.LayerNorm.weight \t tensor([0.7998, 0.7687, 0.7521,  ..., 0.7481, 0.6910, 0.7214])\n",
      "roberta.encoder.layer.19.attention.output.LayerNorm.bias \t tensor([-1.3628e-02, -1.0097e-01, -1.7698e-02,  ..., -6.1844e-02,\n",
      "         2.8259e-02, -3.8070e-05])\n",
      "roberta.encoder.layer.19.intermediate.dense.weight \t tensor([[ 0.0050,  0.0049, -0.0121,  ...,  0.0321,  0.0074, -0.0252],\n",
      "        [-0.0575,  0.0130, -0.0080,  ...,  0.0161, -0.0431, -0.0328],\n",
      "        [ 0.0069,  0.0541, -0.0293,  ...,  0.0267,  0.0327, -0.0020],\n",
      "        ...,\n",
      "        [ 0.0292, -0.0094,  0.0345,  ...,  0.0025,  0.0233, -0.0017],\n",
      "        [ 0.0141,  0.0038, -0.0192,  ..., -0.0113, -0.0284,  0.0208],\n",
      "        [-0.0204, -0.0088, -0.0391,  ...,  0.0455, -0.0188, -0.0362]])\n",
      "roberta.encoder.layer.19.intermediate.dense.bias \t tensor([-4.6449e-05, -3.8015e-02, -3.6636e-02,  ..., -3.7649e-03,\n",
      "        -8.4749e-02, -1.6498e-02])\n",
      "roberta.encoder.layer.19.output.dense.weight \t tensor([[-0.0285, -0.0201,  0.0030,  ...,  0.0094, -0.0122,  0.0253],\n",
      "        [ 0.0394, -0.1076,  0.0245,  ...,  0.0048, -0.0067, -0.0470],\n",
      "        [ 0.0245, -0.0004, -0.0409,  ...,  0.0209,  0.0045, -0.0753],\n",
      "        ...,\n",
      "        [-0.0452, -0.0777,  0.0249,  ..., -0.0205,  0.0196, -0.0329],\n",
      "        [ 0.0001,  0.0105,  0.0138,  ..., -0.0251,  0.0019, -0.0214],\n",
      "        [ 0.0457, -0.0090, -0.0209,  ...,  0.0360, -0.0057, -0.0399]])\n",
      "roberta.encoder.layer.19.output.dense.bias \t tensor([ 0.0225, -0.0179, -0.0081,  ..., -0.0444,  0.0234,  0.0258])\n",
      "roberta.encoder.layer.19.output.LayerNorm.weight \t tensor([0.8217, 0.7833, 0.7985,  ..., 0.7978, 0.7692, 0.7773])\n",
      "roberta.encoder.layer.19.output.LayerNorm.bias \t tensor([ 0.0056,  0.0335, -0.0153,  ..., -0.0248, -0.0178,  0.0127])\n",
      "roberta.encoder.layer.20.attention.self.query.weight \t tensor([[-0.0603, -0.0248, -0.0417,  ..., -0.0001,  0.0354, -0.0784],\n",
      "        [ 0.0545, -0.0465, -0.0159,  ..., -0.0060, -0.0036, -0.0251],\n",
      "        [ 0.0697, -0.0740,  0.0621,  ...,  0.0013,  0.0413,  0.0115],\n",
      "        ...,\n",
      "        [-0.0275,  0.0258, -0.0158,  ...,  0.0703,  0.0050, -0.0488],\n",
      "        [-0.0301,  0.0225, -0.0788,  ..., -0.0516,  0.0053,  0.0206],\n",
      "        [-0.0424,  0.0122,  0.0095,  ...,  0.0062,  0.0063,  0.0083]])\n",
      "roberta.encoder.layer.20.attention.self.query.bias \t tensor([-0.0021, -0.0102, -0.1026,  ..., -0.1044,  0.0346, -0.0105])\n",
      "roberta.encoder.layer.20.attention.self.key.weight \t tensor([[-0.0176, -0.0115,  0.0398,  ..., -0.0009, -0.0046,  0.0112],\n",
      "        [ 0.0061, -0.0457, -0.0110,  ...,  0.0377, -0.0191,  0.0027],\n",
      "        [ 0.0211,  0.0103, -0.0073,  ...,  0.0530,  0.0284,  0.0156],\n",
      "        ...,\n",
      "        [-0.0154, -0.0444, -0.0707,  ...,  0.0243, -0.0196, -0.0204],\n",
      "        [-0.0357, -0.0672, -0.0218,  ..., -0.0168, -0.0156, -0.0326],\n",
      "        [-0.0282, -0.0274, -0.0147,  ..., -0.0215, -0.0362,  0.0701]])\n",
      "roberta.encoder.layer.20.attention.self.key.bias \t tensor([ 8.4070e-05,  1.0612e-04, -1.5574e-03,  ...,  8.7434e-03,\n",
      "        -6.3020e-03,  8.7743e-03])\n",
      "roberta.encoder.layer.20.attention.self.value.weight \t tensor([[-0.0057, -0.0397,  0.0480,  ...,  0.0052, -0.0024, -0.0387],\n",
      "        [-0.0030,  0.0525, -0.0843,  ...,  0.0045, -0.0229, -0.0127],\n",
      "        [ 0.0141,  0.0049, -0.0003,  ...,  0.0257,  0.0588, -0.0009],\n",
      "        ...,\n",
      "        [-0.0107,  0.0231,  0.0734,  ..., -0.0243, -0.0408,  0.0269],\n",
      "        [ 0.0140,  0.0324, -0.0539,  ..., -0.0424,  0.0242, -0.0255],\n",
      "        [-0.0034, -0.0138, -0.0076,  ..., -0.0742,  0.0375, -0.0162]])\n",
      "roberta.encoder.layer.20.attention.self.value.bias \t tensor([-0.0051,  0.0003, -0.0006,  ..., -0.0050, -0.0083,  0.0123])\n",
      "roberta.encoder.layer.20.attention.output.dense.weight \t tensor([[-0.0105,  0.0376, -0.0460,  ..., -0.0124, -0.0052,  0.0166],\n",
      "        [ 0.0200,  0.0302, -0.0526,  ..., -0.1044, -0.0234, -0.0086],\n",
      "        [-0.0042,  0.0259, -0.0151,  ..., -0.0367,  0.0487, -0.0005],\n",
      "        ...,\n",
      "        [ 0.0232,  0.0117,  0.0077,  ...,  0.0135,  0.0084, -0.0140],\n",
      "        [ 0.0023,  0.0028, -0.0121,  ...,  0.0043,  0.0245, -0.0025],\n",
      "        [ 0.0176, -0.0042, -0.0217,  ..., -0.0213, -0.0021, -0.0341]])\n",
      "roberta.encoder.layer.20.attention.output.dense.bias \t tensor([-0.0030, -0.0215,  0.0046,  ...,  0.0136,  0.0031, -0.0111])\n",
      "roberta.encoder.layer.20.attention.output.LayerNorm.weight \t tensor([0.7589, 0.7571, 0.7325,  ..., 0.7391, 0.6572, 0.7075])\n",
      "roberta.encoder.layer.20.attention.output.LayerNorm.bias \t tensor([-0.0124, -0.0791, -0.0319,  ..., -0.0693,  0.0154,  0.0457])\n",
      "roberta.encoder.layer.20.intermediate.dense.weight \t tensor([[ 0.0216,  0.0166, -0.0227,  ..., -0.0055, -0.0188, -0.0191],\n",
      "        [ 0.0011, -0.0026,  0.0046,  ..., -0.0338,  0.0003, -0.0261],\n",
      "        [-0.0050,  0.0652, -0.0264,  ...,  0.0696,  0.0150,  0.0075],\n",
      "        ...,\n",
      "        [-0.0681, -0.0032, -0.0196,  ...,  0.0009,  0.0033,  0.0297],\n",
      "        [ 0.0230,  0.0735,  0.0922,  ..., -0.0515,  0.0284,  0.0422],\n",
      "        [ 0.0170,  0.0204, -0.0185,  ..., -0.0377,  0.0056,  0.0408]])\n",
      "roberta.encoder.layer.20.intermediate.dense.bias \t tensor([-0.0201, -0.0086, -0.0203,  ..., -0.0113, -0.0516, -0.0085])\n",
      "roberta.encoder.layer.20.output.dense.weight \t tensor([[-0.0317,  0.0100,  0.0233,  ...,  0.0092, -0.0316, -0.0130],\n",
      "        [ 0.0057,  0.0151, -0.0450,  ...,  0.0348,  0.0293, -0.0023],\n",
      "        [-0.0235, -0.0126, -0.0035,  ..., -0.0298,  0.0100, -0.0102],\n",
      "        ...,\n",
      "        [ 0.0216, -0.0422, -0.0083,  ..., -0.0065, -0.0427, -0.0178],\n",
      "        [ 0.0156,  0.0277, -0.0236,  ..., -0.0100,  0.0137,  0.0314],\n",
      "        [ 0.0396,  0.0647,  0.0172,  ..., -0.0231,  0.0035, -0.0014]])\n",
      "roberta.encoder.layer.20.output.dense.bias \t tensor([ 0.0034, -0.0404, -0.0396,  ..., -0.0465,  0.0059,  0.0545])\n",
      "roberta.encoder.layer.20.output.LayerNorm.weight \t tensor([0.8425, 0.7932, 0.8065,  ..., 0.8185, 0.7726, 0.7926])\n",
      "roberta.encoder.layer.20.output.LayerNorm.bias \t tensor([-0.0073,  0.0162, -0.0315,  ..., -0.0163, -0.0055,  0.0089])\n",
      "roberta.encoder.layer.21.attention.self.query.weight \t tensor([[ 0.0473, -0.0430, -0.0007,  ...,  0.0516, -0.0375, -0.0148],\n",
      "        [ 0.0340, -0.0244, -0.0556,  ..., -0.0434,  0.0381,  0.0229],\n",
      "        [ 0.0622, -0.0238, -0.0270,  ...,  0.0612,  0.1106,  0.0164],\n",
      "        ...,\n",
      "        [ 0.0845,  0.0243,  0.0229,  ...,  0.0871, -0.0704,  0.0034],\n",
      "        [-0.0078,  0.0133, -0.0379,  ...,  0.0073, -0.0041, -0.0716],\n",
      "        [ 0.0676,  0.0114,  0.0290,  ...,  0.0845, -0.0128, -0.0223]])\n",
      "roberta.encoder.layer.21.attention.self.query.bias \t tensor([-0.0554, -0.1006,  0.0081,  ..., -0.0033,  0.1034, -0.0115])\n",
      "roberta.encoder.layer.21.attention.self.key.weight \t tensor([[-0.0814, -0.0488, -0.0598,  ..., -0.0170,  0.0768,  0.0096],\n",
      "        [ 0.0010, -0.0996, -0.0803,  ..., -0.0262, -0.0347, -0.0397],\n",
      "        [ 0.0705, -0.0221,  0.0444,  ...,  0.0309,  0.0400, -0.0274],\n",
      "        ...,\n",
      "        [-0.0103,  0.0330,  0.0262,  ...,  0.0233, -0.0357, -0.0471],\n",
      "        [-0.0107,  0.0250, -0.0484,  ..., -0.0091,  0.0049,  0.0326],\n",
      "        [-0.0311,  0.0285,  0.0336,  ..., -0.0388, -0.0242,  0.0281]])\n",
      "roberta.encoder.layer.21.attention.self.key.bias \t tensor([ 0.0003, -0.0055,  0.0023,  ...,  0.0032,  0.0076,  0.0021])\n",
      "roberta.encoder.layer.21.attention.self.value.weight \t tensor([[-0.0115, -0.0342,  0.0170,  ...,  0.0212, -0.0575, -0.0400],\n",
      "        [ 0.0173,  0.0321,  0.0235,  ...,  0.0088, -0.0191,  0.0390],\n",
      "        [-0.0137, -0.0071, -0.0191,  ..., -0.0043,  0.0199, -0.0046],\n",
      "        ...,\n",
      "        [-0.0050,  0.0104,  0.0040,  ...,  0.0139, -0.0485, -0.0097],\n",
      "        [ 0.0323,  0.0279,  0.0161,  ..., -0.0368, -0.0815, -0.0005],\n",
      "        [ 0.0051, -0.0096, -0.0142,  ..., -0.0147, -0.0305, -0.0057]])\n",
      "roberta.encoder.layer.21.attention.self.value.bias \t tensor([-0.0069, -0.0257,  0.0086,  ..., -0.0063,  0.0043,  0.0106])\n",
      "roberta.encoder.layer.21.attention.output.dense.weight \t tensor([[-0.0070,  0.0136, -0.0285,  ...,  0.0022, -0.0135,  0.0200],\n",
      "        [ 0.0082,  0.0164, -0.0356,  ..., -0.0215,  0.0250,  0.0175],\n",
      "        [-0.0038, -0.0465,  0.0294,  ..., -0.0381, -0.0043,  0.0342],\n",
      "        ...,\n",
      "        [ 0.0016,  0.0115,  0.0435,  ..., -0.0377,  0.0197,  0.0101],\n",
      "        [ 0.0085, -0.0120,  0.0220,  ..., -0.0250, -0.0223,  0.0015],\n",
      "        [-0.0031, -0.0264,  0.0651,  ...,  0.0230,  0.0336, -0.0155]])\n",
      "roberta.encoder.layer.21.attention.output.dense.bias \t tensor([ 0.0141, -0.0213, -0.0162,  ...,  0.0464, -0.0220,  0.0199])\n",
      "roberta.encoder.layer.21.attention.output.LayerNorm.weight \t tensor([0.7309, 0.7325, 0.7200,  ..., 0.7337, 0.6556, 0.6876])\n",
      "roberta.encoder.layer.21.attention.output.LayerNorm.bias \t tensor([-0.0271, -0.0557, -0.0459,  ..., -0.0407,  0.0093,  0.0291])\n",
      "roberta.encoder.layer.21.intermediate.dense.weight \t tensor([[ 0.0651,  0.0227,  0.0099,  ..., -0.0508, -0.0321,  0.0062],\n",
      "        [ 0.0146,  0.0167, -0.0208,  ..., -0.0229,  0.0022, -0.0051],\n",
      "        [ 0.0245,  0.0083, -0.0256,  ...,  0.0051, -0.0509, -0.0207],\n",
      "        ...,\n",
      "        [ 0.0207,  0.0273, -0.0107,  ..., -0.0153,  0.0327, -0.0073],\n",
      "        [-0.0128, -0.0213,  0.0406,  ..., -0.0140, -0.0144, -0.0514],\n",
      "        [ 0.0419,  0.0007,  0.0863,  ..., -0.0479, -0.0610,  0.0113]])\n",
      "roberta.encoder.layer.21.intermediate.dense.bias \t tensor([ 0.0035, -0.0112,  0.0101,  ..., -0.0189, -0.0127, -0.0144])\n",
      "roberta.encoder.layer.21.output.dense.weight \t tensor([[ 0.0141,  0.0185, -0.0175,  ...,  0.0100, -0.0085, -0.0384],\n",
      "        [-0.0734,  0.0368,  0.0010,  ..., -0.0030,  0.0093,  0.0099],\n",
      "        [-0.0472, -0.0160, -0.0359,  ...,  0.0160, -0.0201,  0.0887],\n",
      "        ...,\n",
      "        [ 0.0036, -0.0213, -0.0077,  ...,  0.0013,  0.0079,  0.0245],\n",
      "        [ 0.0180, -0.0221, -0.0568,  ...,  0.0361, -0.0188, -0.0274],\n",
      "        [-0.0014, -0.0044, -0.0204,  ...,  0.0242, -0.0245,  0.0124]])\n",
      "roberta.encoder.layer.21.output.dense.bias \t tensor([-0.0182, -0.0524, -0.0601,  ..., -0.0310,  0.0088,  0.0373])\n",
      "roberta.encoder.layer.21.output.LayerNorm.weight \t tensor([0.8056, 0.7939, 0.8061,  ..., 0.8080, 0.7804, 0.7957])\n",
      "roberta.encoder.layer.21.output.LayerNorm.bias \t tensor([-0.0095,  0.0220, -0.0546,  ...,  0.0094,  0.0003,  0.0020])\n",
      "roberta.encoder.layer.22.attention.self.query.weight \t tensor([[ 0.0170,  0.0296,  0.0001,  ..., -0.0284,  0.0190,  0.0492],\n",
      "        [ 0.0062,  0.0151,  0.0215,  ...,  0.0137,  0.0156, -0.0300],\n",
      "        [ 0.0392,  0.0173, -0.0127,  ..., -0.0568,  0.0270,  0.0150],\n",
      "        ...,\n",
      "        [ 0.0190,  0.0141,  0.0116,  ...,  0.0226,  0.0451, -0.0301],\n",
      "        [ 0.0012,  0.0051,  0.0248,  ..., -0.0257, -0.0196,  0.1002],\n",
      "        [ 0.0029, -0.0265,  0.0366,  ..., -0.0146,  0.0219, -0.0221]])\n",
      "roberta.encoder.layer.22.attention.self.query.bias \t tensor([ 0.0918, -0.1169, -0.1641,  ..., -0.0157, -0.0895,  0.0060])\n",
      "roberta.encoder.layer.22.attention.self.key.weight \t tensor([[-0.0283,  0.0073,  0.0412,  ..., -0.0072, -0.0063,  0.0156],\n",
      "        [-0.0011,  0.0131,  0.0453,  ...,  0.0153,  0.0009,  0.0151],\n",
      "        [-0.0368, -0.0162, -0.0054,  ...,  0.0044,  0.0678,  0.0058],\n",
      "        ...,\n",
      "        [-0.0141, -0.0419,  0.0243,  ..., -0.0135, -0.0122, -0.0070],\n",
      "        [ 0.0229, -0.0073, -0.0195,  ..., -0.0569,  0.0254,  0.0009],\n",
      "        [-0.0147,  0.0100, -0.0326,  ...,  0.0483,  0.0602, -0.0175]])\n",
      "roberta.encoder.layer.22.attention.self.key.bias \t tensor([ 0.0025, -0.0034, -0.0022,  ..., -0.0029, -0.0060,  0.0011])\n",
      "roberta.encoder.layer.22.attention.self.value.weight \t tensor([[ 0.0158,  0.0185,  0.0017,  ...,  0.0114, -0.0369,  0.0071],\n",
      "        [ 0.0358,  0.0364,  0.0582,  ...,  0.0522, -0.0008, -0.0129],\n",
      "        [-0.0311,  0.0224, -0.0607,  ..., -0.0177, -0.0108, -0.0171],\n",
      "        ...,\n",
      "        [ 0.0004, -0.0156,  0.0234,  ..., -0.0020,  0.0393, -0.0127],\n",
      "        [ 0.0124, -0.0319, -0.0397,  ..., -0.0074, -0.0351,  0.0066],\n",
      "        [-0.0305, -0.0488, -0.0114,  ..., -0.0123, -0.0090, -0.0030]])\n",
      "roberta.encoder.layer.22.attention.self.value.bias \t tensor([-0.0004, -0.0068, -0.0124,  ..., -0.0107, -0.0057,  0.0054])\n",
      "roberta.encoder.layer.22.attention.output.dense.weight \t tensor([[-0.0052,  0.0331, -0.0438,  ...,  0.0183,  0.0454,  0.0042],\n",
      "        [-0.0332, -0.0057, -0.0096,  ...,  0.0023, -0.0284, -0.0638],\n",
      "        [ 0.0153, -0.0559,  0.0124,  ...,  0.0131, -0.0308, -0.0156],\n",
      "        ...,\n",
      "        [ 0.0243,  0.0452,  0.0311,  ...,  0.0143, -0.0268,  0.0263],\n",
      "        [ 0.0197,  0.0200,  0.0008,  ...,  0.0630, -0.0159, -0.0247],\n",
      "        [-0.0525,  0.0034, -0.0037,  ...,  0.0033, -0.0069, -0.0023]])\n",
      "roberta.encoder.layer.22.attention.output.dense.bias \t tensor([-0.0055,  0.0012,  0.0226,  ...,  0.0057, -0.0639,  0.0261])\n",
      "roberta.encoder.layer.22.attention.output.LayerNorm.weight \t tensor([0.7007, 0.7200, 0.7142,  ..., 0.7109, 0.6841, 0.6878])\n",
      "roberta.encoder.layer.22.attention.output.LayerNorm.bias \t tensor([-0.0432, -0.0431, -0.0455,  ..., -0.0416,  0.0309,  0.0015])\n",
      "roberta.encoder.layer.22.intermediate.dense.weight \t tensor([[-0.0069,  0.0905,  0.0507,  ..., -0.0357, -0.0618,  0.0266],\n",
      "        [ 0.0088, -0.0148, -0.0157,  ...,  0.0566, -0.0103,  0.0213],\n",
      "        [ 0.0068, -0.0251, -0.0268,  ..., -0.0246, -0.0494,  0.0163],\n",
      "        ...,\n",
      "        [ 0.0215,  0.0015, -0.0082,  ..., -0.0466,  0.0237,  0.0081],\n",
      "        [ 0.0356, -0.0115, -0.0189,  ..., -0.0310, -0.0652, -0.0521],\n",
      "        [-0.0193,  0.0088,  0.0487,  ...,  0.0067, -0.0264,  0.0067]])\n",
      "roberta.encoder.layer.22.intermediate.dense.bias \t tensor([-0.0549, -0.0419, -0.0174,  ..., -0.0420, -0.0127, -0.1414])\n",
      "roberta.encoder.layer.22.output.dense.weight \t tensor([[ 0.0138, -0.0295,  0.0291,  ...,  0.0135,  0.0470,  0.0633],\n",
      "        [ 0.0027,  0.0366,  0.0492,  ...,  0.1016, -0.0190, -0.0219],\n",
      "        [ 0.0153,  0.0083,  0.0285,  ...,  0.0356, -0.0037, -0.0499],\n",
      "        ...,\n",
      "        [-0.0503, -0.0127,  0.0376,  ..., -0.0581, -0.0057,  0.0182],\n",
      "        [-0.0443, -0.0355, -0.0344,  ..., -0.0602, -0.0039, -0.0387],\n",
      "        [-0.0011, -0.0133, -0.0524,  ...,  0.0503,  0.0078, -0.0419]])\n",
      "roberta.encoder.layer.22.output.dense.bias \t tensor([-4.3579e-02, -3.2694e-02, -6.2702e-02,  ..., -4.5669e-02,\n",
      "        -2.1745e-05,  3.9817e-03])\n",
      "roberta.encoder.layer.22.output.LayerNorm.weight \t tensor([0.8151, 0.8037, 0.7947,  ..., 0.8117, 0.7836, 0.7914])\n",
      "roberta.encoder.layer.22.output.LayerNorm.bias \t tensor([-0.0331,  0.0065, -0.0177,  ..., -0.0343, -0.0397, -0.0157])\n",
      "roberta.encoder.layer.23.attention.self.query.weight \t tensor([[-0.0164, -0.0139,  0.0077,  ..., -0.0097, -0.0134,  0.0243],\n",
      "        [ 0.0328,  0.0322,  0.0427,  ...,  0.0335, -0.0045,  0.0259],\n",
      "        [-0.0155,  0.0157,  0.0031,  ...,  0.0327, -0.0043, -0.0681],\n",
      "        ...,\n",
      "        [-0.0050, -0.0235,  0.0281,  ..., -0.0677, -0.0074,  0.0591],\n",
      "        [ 0.0263, -0.0185, -0.0428,  ...,  0.0563,  0.0070, -0.0928],\n",
      "        [-0.0530,  0.0681,  0.0086,  ...,  0.0006,  0.0424, -0.0179]])\n",
      "roberta.encoder.layer.23.attention.self.query.bias \t tensor([ 0.0074,  0.0605,  0.0341,  ...,  0.0402,  0.0187, -0.0226])\n",
      "roberta.encoder.layer.23.attention.self.key.weight \t tensor([[-0.0004, -0.0827, -0.0584,  ...,  0.0144,  0.0282,  0.0339],\n",
      "        [-0.0417, -0.0228, -0.0235,  ..., -0.0401, -0.0154,  0.0311],\n",
      "        [-0.0351,  0.0402, -0.0374,  ...,  0.0354, -0.0267, -0.0531],\n",
      "        ...,\n",
      "        [-0.0129, -0.0288, -0.0243,  ..., -0.0070, -0.0476, -0.0104],\n",
      "        [-0.0434,  0.0358, -0.0426,  ...,  0.0100,  0.1061, -0.0671],\n",
      "        [ 0.0245,  0.0172, -0.0012,  ..., -0.0599, -0.0009,  0.0632]])\n",
      "roberta.encoder.layer.23.attention.self.key.bias \t tensor([ 0.0002, -0.0021,  0.0007,  ..., -0.0017,  0.0036, -0.0001])\n",
      "roberta.encoder.layer.23.attention.self.value.weight \t tensor([[ 4.4595e-02, -3.7088e-06, -4.1582e-02,  ...,  1.6226e-02,\n",
      "          9.8740e-03,  3.1213e-03],\n",
      "        [ 2.1734e-02,  1.5388e-02, -6.0028e-02,  ...,  4.2352e-02,\n",
      "          3.7954e-02, -5.6511e-02],\n",
      "        [-4.1924e-02,  3.3975e-02,  5.1923e-02,  ...,  5.4734e-02,\n",
      "          2.0394e-02, -4.0701e-02],\n",
      "        ...,\n",
      "        [ 2.7960e-02, -2.5796e-02, -1.1838e-02,  ...,  3.8456e-02,\n",
      "          1.8813e-02, -2.0181e-02],\n",
      "        [-1.6668e-02,  6.8531e-03, -4.1610e-02,  ...,  7.8211e-02,\n",
      "          3.1744e-02, -1.7016e-03],\n",
      "        [ 2.7885e-03, -2.1322e-02,  3.5078e-02,  ..., -1.2767e-02,\n",
      "          1.2361e-02,  9.6464e-03]])\n",
      "roberta.encoder.layer.23.attention.self.value.bias \t tensor([-0.0058, -0.0018,  0.0031,  ..., -0.0106, -0.0050, -0.0133])\n",
      "roberta.encoder.layer.23.attention.output.dense.weight \t tensor([[ 0.0276,  0.0159, -0.0824,  ...,  0.0360, -0.0038, -0.0008],\n",
      "        [ 0.0055,  0.0094, -0.0054,  ..., -0.0251,  0.0317, -0.0165],\n",
      "        [-0.0469, -0.0062,  0.0270,  ..., -0.0127,  0.0041, -0.0136],\n",
      "        ...,\n",
      "        [ 0.0118, -0.0297,  0.0482,  ...,  0.0097,  0.0294,  0.0611],\n",
      "        [ 0.0237,  0.0355, -0.0306,  ..., -0.0075,  0.0277,  0.0059],\n",
      "        [-0.0402, -0.0149,  0.0008,  ...,  0.0138, -0.0241,  0.0115]])\n",
      "roberta.encoder.layer.23.attention.output.dense.bias \t tensor([-0.0649, -0.0424, -0.0698,  ...,  0.0144, -0.0437,  0.0881])\n",
      "roberta.encoder.layer.23.attention.output.LayerNorm.weight \t tensor([0.7099, 0.7046, 0.7342,  ..., 0.7085, 0.6851, 0.7063])\n",
      "roberta.encoder.layer.23.attention.output.LayerNorm.bias \t tensor([ 0.0205, -0.0714,  0.0080,  ..., -0.0403,  0.0114, -0.0514])\n",
      "roberta.encoder.layer.23.intermediate.dense.weight \t tensor([[-0.0074,  0.0070,  0.0061,  ..., -0.0271,  0.0168, -0.0157],\n",
      "        [ 0.0234,  0.0127,  0.0381,  ...,  0.0314, -0.0631,  0.0237],\n",
      "        [ 0.0055,  0.0284, -0.0503,  ..., -0.0475, -0.0264, -0.0022],\n",
      "        ...,\n",
      "        [-0.0136,  0.0040,  0.0590,  ..., -0.0222, -0.0861,  0.0362],\n",
      "        [-0.0182, -0.0631,  0.0181,  ...,  0.0030, -0.0187,  0.0274],\n",
      "        [ 0.0149, -0.0552,  0.0333,  ...,  0.0105,  0.0067,  0.0156]])\n",
      "roberta.encoder.layer.23.intermediate.dense.bias \t tensor([ 0.0102, -0.0143, -0.0066,  ...,  0.0122, -0.0204, -0.0159])\n",
      "roberta.encoder.layer.23.output.dense.weight \t tensor([[ 0.0575, -0.0553,  0.0062,  ..., -0.0311, -0.0334, -0.0260],\n",
      "        [ 0.0207,  0.0060, -0.0342,  ...,  0.0173, -0.0153,  0.0061],\n",
      "        [-0.0002,  0.0178,  0.0281,  ...,  0.0226, -0.0032, -0.0265],\n",
      "        ...,\n",
      "        [-0.0054, -0.0126, -0.0086,  ..., -0.0072, -0.0605,  0.0015],\n",
      "        [-0.0602, -0.0279,  0.0031,  ..., -0.0336,  0.0071,  0.0059],\n",
      "        [-0.0433,  0.0106,  0.0127,  ...,  0.0733,  0.0071, -0.0420]])\n",
      "roberta.encoder.layer.23.output.dense.bias \t tensor([-0.0189, -0.0296,  0.0108,  ..., -0.0403,  0.0126, -0.0088])\n",
      "roberta.encoder.layer.23.output.LayerNorm.weight \t tensor([0.6744, 0.6513, 0.6649,  ..., 0.6478, 0.6582, 0.6321])\n",
      "roberta.encoder.layer.23.output.LayerNorm.bias \t tensor([-0.0077, -0.0592, -0.0320,  ..., -0.0565,  0.0081, -0.0125])\n",
      "classifier.dense.weight \t tensor([[ 0.0084, -0.0144, -0.0222,  ...,  0.0321, -0.0257, -0.0243],\n",
      "        [ 0.0026,  0.0353,  0.0211,  ..., -0.0029, -0.0039,  0.0054],\n",
      "        [ 0.0030,  0.0009,  0.0254,  ..., -0.0059,  0.0052,  0.0164],\n",
      "        ...,\n",
      "        [-0.0084, -0.0324, -0.0071,  ..., -0.0039, -0.0037, -0.0136],\n",
      "        [ 0.0046,  0.0120,  0.0073,  ..., -0.0157,  0.0098, -0.0157],\n",
      "        [ 0.0169, -0.0097,  0.0256,  ..., -0.0120, -0.0146,  0.0006]])\n",
      "classifier.dense.bias \t tensor([ 0.0007, -0.0009,  0.0011,  ..., -0.0006, -0.0013, -0.0002])\n",
      "classifier.out_proj.weight \t tensor([[ 1.7745e-02, -8.6851e-03,  1.4538e-03,  ..., -1.5077e-02,\n",
      "         -1.7393e-02, -4.3176e-06],\n",
      "        [ 1.7862e-03, -2.9682e-02,  1.3582e-02,  ...,  1.4828e-02,\n",
      "         -4.2509e-03,  1.6035e-02],\n",
      "        [-2.4194e-02, -2.6611e-02,  1.6020e-02,  ...,  8.8371e-03,\n",
      "          7.9827e-03, -4.3983e-02],\n",
      "        ...,\n",
      "        [ 5.0251e-03,  1.3849e-02, -1.2159e-02,  ...,  2.9464e-02,\n",
      "          8.7311e-03,  9.0957e-04],\n",
      "        [-8.5487e-03, -1.0226e-02, -6.3798e-03,  ...,  2.5401e-02,\n",
      "          4.1043e-03, -8.9916e-03],\n",
      "        [ 6.4446e-03,  7.7544e-03,  1.8857e-02,  ..., -2.7923e-02,\n",
      "         -1.5454e-02, -6.3675e-03]])\n",
      "classifier.out_proj.bias \t tensor([ 2.9605e-03,  4.6961e-04, -8.4594e-06, -6.9796e-04,  5.4479e-05,\n",
      "        -9.2235e-05,  8.5786e-04,  3.3725e-04, -1.2651e-03, -3.1803e-03,\n",
      "        -8.5797e-04, -1.0520e-03, -5.2068e-05, -1.0505e-03, -4.4277e-04,\n",
      "         3.6767e-04, -1.3229e-03, -6.6017e-05, -1.4647e-03, -1.9180e-03,\n",
      "         4.5921e-04, -6.4303e-04, -2.7086e-03, -2.7642e-03, -1.2616e-03,\n",
      "        -2.6314e-04, -1.4259e-03, -3.1551e-03, -1.3353e-03, -2.5598e-03])\n"
     ]
    }
   ],
   "source": [
    "# 모델의 state_dict 출력\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model[0].state_dict():\n",
    "    print(param_tensor, \"\\t\", model[0].state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de3e2a01-fee1-45ac-bc1c-797a273805b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model = model[0]\n",
    "weighted_model.save_pretrained(f\"./best_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ea1dd-188e-4b40-af58-a1e9adf45956",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
